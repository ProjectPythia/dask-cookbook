{"version":"1","records":[{"hierarchy":{"lvl1":"Dask Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Dask Cookbook"},"content":"\n\n\n\n\n\n\n\nThis Project Pythia Cookbook provides a comprehensive guide to understanding the basic concepts and collections of Dask as well as its integration with Xarray.\nDask is a parallel computing library that allows you to scale your computations to multiple cores or even clusters, while Xarray is a library that enables working with labelled multi-dimensional arrays, with a focus on working with netCDF datasets.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Motivation"},"content":"The motivation behind this repository is to provide a clear and concise resource for anyone looking to learn about the basic concepts of Dask and its integration with Xarray. By providing step-by-step tutorials, we hope to make it easy for users to understand the fundamental concepts of parallel computing and distributed data processing, as well as how to apply them in practice using Dask and Dask+Xarray.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Authors"},"content":"Negin Sobhani, \n\nBrian Vanderwende, \n\nDeepak Cherian, and \n\nBen Kirk","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"Dask Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"Dask Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":" \n\n \n\n","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Note on Content Origin"},"type":"lvl2","url":"/#note-on-content-origin","position":8},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Note on Content Origin"},"content":"This cookbook is derived from the extensive material used in the NCAR tutorial, \n\n“Using Dask on HPC systems”, which was held in February 2023. The NCAR tutorial series also includes an in-depth exploration and practical use cases of Dask on HPC systems and best practices for Dask on HPC. For the complete set of NCAR tutorial materials, including these additional insights\non Dask on HPC, please refer to the main NCAR tutorial content available \n\nhere.","type":"content","url":"/#note-on-content-origin","position":9},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":10},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Structure"},"content":"In the first chapter of this cookbook, we provide step-by-step tutorials on the basic concepts of Dask, including Dask arrays and Dask dataframes, which are powerful tools for parallel computing and distributed data processing. We explain the key differences between these Dask data structures and their counterparts in NumPy and Pandas.\n\nIn the second chapter of the repository, we move on to more advanced topics, such as distributed computing and Dask+Xarray integration. We provide examples of how to use Dask+Xarray to efficiently work with large, labelled multi-dimensional datasets.\nFinally, we will discuss some best practices regarding Dask+Xarray.","type":"content","url":"/#structure","position":11},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":12},{"hierarchy":{"lvl1":"Dask Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":13},{"hierarchy":{"lvl1":"Dask Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":14},{"hierarchy":{"lvl1":"Dask Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":15},{"hierarchy":{"lvl1":"Dask Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":16},{"hierarchy":{"lvl1":"Dask Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/dask-cookbook repository: git clone https://github.com/ProjectPythia/dask-cookbook.git\n\nMove into the dask-cookbook directorycd dask-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate dask-cookbook\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":17},{"hierarchy":{"lvl1":"Dask Cookbook","lvl3":"Acknowledgments","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#acknowledgments","position":18},{"hierarchy":{"lvl1":"Dask Cookbook","lvl3":"Acknowledgments","lvl2":"Running the Notebooks"},"content":"NCAR CISL/CSG Team\n\nESDS Initiative","type":"content","url":"/#acknowledgments","position":19},{"hierarchy":{"lvl1":"Dask Overview"},"type":"lvl1","url":"/notebooks/dask-overview","position":0},{"hierarchy":{"lvl1":"Dask Overview"},"content":"","type":"content","url":"/notebooks/dask-overview","position":1},{"hierarchy":{"lvl1":"Dask Overview"},"type":"lvl1","url":"/notebooks/dask-overview#dask-overview","position":2},{"hierarchy":{"lvl1":"Dask Overview"},"content":"","type":"content","url":"/notebooks/dask-overview#dask-overview","position":3},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"In this tutorial, you learn:"},"type":"lvl3","url":"/notebooks/dask-overview#in-this-tutorial-you-learn","position":4},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"In this tutorial, you learn:"},"content":"What is Dask?\n\nWhy Dask in Geosciences?\n\nDask Data Structures and Schedulers\n\nWhen to use Dask?\n\n","type":"content","url":"/notebooks/dask-overview#in-this-tutorial-you-learn","position":5},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"Introduction"},"type":"lvl2","url":"/notebooks/dask-overview#introduction","position":6},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"Introduction"},"content":"Complex data structures enable data science in Python. For example:\n\nNumPy arrays\n\nPandas series and dataframes\n\nXArray datasets\n\nBut datasets are getting larger all of the time! What if my dataset is too big to fit into memory, or it takes too long to complete an analysis?","type":"content","url":"/notebooks/dask-overview#introduction","position":7},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"What is Dask?"},"type":"lvl2","url":"/notebooks/dask-overview#what-is-dask","position":8},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"What is Dask?"},"content":"Dask is an open-source Python library for parallel and distributed computing that scales the existing Python ecosystem.\n\nDask was developed to scale Python packages such as Numpy, Pandas, and Xarray to multi-core machines and distributed clusters when datasets exceed memory.","type":"content","url":"/notebooks/dask-overview#what-is-dask","position":9},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"Why Dask?"},"type":"lvl2","url":"/notebooks/dask-overview#why-dask","position":10},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"Why Dask?"},"content":"","type":"content","url":"/notebooks/dask-overview#why-dask","position":11},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"Familiar Interface","lvl2":"Why Dask?"},"type":"lvl3","url":"/notebooks/dask-overview#familiar-interface","position":12},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"Familiar Interface","lvl2":"Why Dask?"},"content":"Dask provides interfaces which mimics significant portions of the NumPy and Pandas APIs.\n\nThis means Dask provides ways to parallelize Pandas, Xarray, and Numpy workflows with minimal code rewriting (no massive code-restructure or writing a script in another language).","type":"content","url":"/notebooks/dask-overview#familiar-interface","position":13},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"Scalability","lvl2":"Why Dask?"},"type":"lvl3","url":"/notebooks/dask-overview#scalability","position":14},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"Scalability","lvl2":"Why Dask?"},"content":"Dask is designed to scale well from single machine (laptop) to thousand-node HPC clusters, and on the cloud.\n\nThis allows users to use their existing hardware, or add more machines as needed, to handle increasingly large and complex datasets.","type":"content","url":"/notebooks/dask-overview#scalability","position":15},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"Flexibility","lvl2":"Why Dask?"},"type":"lvl3","url":"/notebooks/dask-overview#flexibility","position":16},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"Flexibility","lvl2":"Why Dask?"},"content":"Dask provides several tools that help with data analysis on large datasets. For example, you can easily wrap your function in dask.delayed decorator to make it run in parallel.\n\nDask provides seamless integration with well-known HPC resource managers and job scheduling systems, including PBS, SLURM, and SGE.","type":"content","url":"/notebooks/dask-overview#flexibility","position":17},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"Built-in Diagnostic Tools","lvl2":"Why Dask?"},"type":"lvl3","url":"/notebooks/dask-overview#built-in-diagnostic-tools","position":18},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"Built-in Diagnostic Tools","lvl2":"Why Dask?"},"content":"Dask provides responsive feedback via the client as well as a real-time interactive diagnostic dashboard to keep users informed on how the computation is progressing.\n\nThis helps users identify and resolve potential issues without waiting for the work to be completed.","type":"content","url":"/notebooks/dask-overview#built-in-diagnostic-tools","position":19},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"First Rule of Dask"},"type":"lvl2","url":"/notebooks/dask-overview#first-rule-of-dask","position":20},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"First Rule of Dask"},"content":"While Dask is a powerful tool for parallel and distributed computing, it is not always the best solution for every problem.\nIn some cases, using Dask may introduce additional complexity and overhead, without providing any substantial benefits in terms of performance or scalability.\n\nKeep in mind the time spent parallelizing and optimizing your workflow when using Dask vs. the time saved because of that parallelization.\n\nConsider how many times you plan to run your code - if only once, is it worth it?\n\n\n\nNOTE: Dask should only be used when necessary.\n\nAvoid Dask if you can easily:\n\nSpeed up your code with use of compiled routines in libraries like NumPy\n\nProfile and optimize your serial code to minimize bottlenecks\n\nRead in a subset of data to gain the insight you need\n\nAnd keep in mind - all of the above steps improve your code whether you end up using Dask or not! \n\n","type":"content","url":"/notebooks/dask-overview#first-rule-of-dask","position":21},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"When to use Dask?"},"type":"lvl2","url":"/notebooks/dask-overview#when-to-use-dask","position":22},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"When to use Dask?"},"content":"Here are some general guidelines for when to use Dask and when to avoid it:","type":"content","url":"/notebooks/dask-overview#when-to-use-dask","position":23},{"hierarchy":{"lvl1":"Dask Overview","lvl4":"Use Dask:","lvl2":"When to use Dask?"},"type":"lvl4","url":"/notebooks/dask-overview#use-dask","position":24},{"hierarchy":{"lvl1":"Dask Overview","lvl4":"Use Dask:","lvl2":"When to use Dask?"},"content":"When you have large datasets that don’t fit into memory on a single machine.\n\nWhen you need to perform parallel computations, such as big data analysis.","type":"content","url":"/notebooks/dask-overview#use-dask","position":25},{"hierarchy":{"lvl1":"Dask Overview","lvl4":"Avoid Dask:","lvl2":"When to use Dask?"},"type":"lvl4","url":"/notebooks/dask-overview#avoid-dask","position":26},{"hierarchy":{"lvl1":"Dask Overview","lvl4":"Avoid Dask:","lvl2":"When to use Dask?"},"content":"When you have small datasets that can be processed efficiently on a single machine.\n\nWhen you don’t need parallel processing, as the overhead of managing a distributed computing environment may not be worth the benefits.\n\nWhen you need to debug or troubleshoot problems, as distributed computing environments can be challenging for debugging. If the problem is complex, using Dask may make debugging more difficult.\n\n","type":"content","url":"/notebooks/dask-overview#avoid-dask","position":27},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"Dask Components"},"type":"lvl2","url":"/notebooks/dask-overview#dask-components","position":28},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"Dask Components"},"content":"Dask is composed of two main parts:","type":"content","url":"/notebooks/dask-overview#dask-components","position":29},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"1.  Dask Collections","lvl2":"Dask Components"},"type":"lvl3","url":"/notebooks/dask-overview#id-1-dask-collections","position":30},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"1.  Dask Collections","lvl2":"Dask Components"},"content":"Dask Collections are the user interfaces we use for parallel and distributed computing with Dask.\n\nDask features different levels of collection types:","type":"content","url":"/notebooks/dask-overview#id-1-dask-collections","position":31},{"hierarchy":{"lvl1":"Dask Overview","lvl4":"High-level collections","lvl3":"1.  Dask Collections","lvl2":"Dask Components"},"type":"lvl4","url":"/notebooks/dask-overview#high-level-collections","position":32},{"hierarchy":{"lvl1":"Dask Overview","lvl4":"High-level collections","lvl3":"1.  Dask Collections","lvl2":"Dask Components"},"content":"Dask provides high-level collections Dask Arrays, Dask DataFrames, and Dask Bags that mimic NumPy, pandas, and lists but can operate in parallel on datasets that don’t fit into memory.\n\nMost of the time, you will probably use one of the following high-level (big) data structures (or an even higher-level derivative type like Xarrays):\n\nCollection\n\nSerial\n\nDask\n\nArrays\n\nnumpy.array\n\ndask.array.from_array\n\nDataframes\n\npandas.read_csv\n\ndask.dataframe.read_csv\n\nUnstructured\n\n[1,2,3]\n\ndask.bag.from_sequence([1,2,3])","type":"content","url":"/notebooks/dask-overview#high-level-collections","position":33},{"hierarchy":{"lvl1":"Dask Overview","lvl4":"Low-level collections","lvl3":"1.  Dask Collections","lvl2":"Dask Components"},"type":"lvl4","url":"/notebooks/dask-overview#low-level-collections","position":34},{"hierarchy":{"lvl1":"Dask Overview","lvl4":"Low-level collections","lvl3":"1.  Dask Collections","lvl2":"Dask Components"},"content":"Dask also features two low-level collection types - delayed and futures.  These collections give users finer control to build custom parallel and distributed computations.\n\ndelayed - run any arbitrary Python function using Dask task parallelism (think looped function calls)\n\nfutures - similar to delayed but allows for concurrent commands in the client script (think backgrounded processes)\n\nThese are very powerfull tools, but it is easy to write something using a delayed function that could be executed faster and more simply using a high-level collection\n\n\n\nImage credit: Dask Contributors","type":"content","url":"/notebooks/dask-overview#low-level-collections","position":35},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"2. Dynamic Task Scheduling","lvl2":"Dask Components"},"type":"lvl3","url":"/notebooks/dask-overview#id-2-dynamic-task-scheduling","position":36},{"hierarchy":{"lvl1":"Dask Overview","lvl3":"2. Dynamic Task Scheduling","lvl2":"Dask Components"},"content":"We can basically think of the Dask scheduler as our task orchestrator.\n\nWhen a computation is submitted, work is segmented into discrete tasks which are assigned to workers by the Dask scheduler.\n\nTo perform work, a scheduler must be assigned resources in the form of a Dask cluster. The cluster consists of the following components:\n\nscheduler : A scheduler creates and manages task graphs and distributes tasks to workers.\n\nworkers : A worker is typically a separate Python process on either the local host or a remote machine. A Dask cluster usually consists of many workers. Basically, a worker is a Python interpretor which will perform work on a subset of our dataset.\n\nclient - A high-level interface that points to the scheduler (often local but not always). A client serves as the entry point for interacting with a Dask scheduler.\n\n\n\nImage credit: Dask Contributors\n\nWe will learn more about Dask Collections and Dynamic Task Scheduling in the next tutorials.\n\n","type":"content","url":"/notebooks/dask-overview#id-2-dynamic-task-scheduling","position":37},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"Useful Resources"},"type":"lvl2","url":"/notebooks/dask-overview#useful-resources","position":38},{"hierarchy":{"lvl1":"Dask Overview","lvl2":"Useful Resources"},"content":"Reference\n\nDocs\n\nExamples\n\nCode\n\nBlog\n\nAsk for help\n\ndask tag on Stack Overflow, for usage questions\n\ngithub issues for bug reports and feature requests\n\ndiscourse forum for general, non-bug, questions and discussion","type":"content","url":"/notebooks/dask-overview#useful-resources","position":39},{"hierarchy":{"lvl1":"Dask Array"},"type":"lvl1","url":"/notebooks/dask-array","position":0},{"hierarchy":{"lvl1":"Dask Array"},"content":"","type":"content","url":"/notebooks/dask-array","position":1},{"hierarchy":{"lvl1":"Dask Array"},"type":"lvl1","url":"/notebooks/dask-array#dask-array","position":2},{"hierarchy":{"lvl1":"Dask Array"},"content":"","type":"content","url":"/notebooks/dask-array#dask-array","position":3},{"hierarchy":{"lvl1":"Dask Array","lvl3":"In this tutorial, you learn:"},"type":"lvl3","url":"/notebooks/dask-array#in-this-tutorial-you-learn","position":4},{"hierarchy":{"lvl1":"Dask Array","lvl3":"In this tutorial, you learn:"},"content":"What is a Dask Array?\n\nBasic concepts and features of Dask Arrays\n\nWorking with Dask arrays\n\nRelated Dask Array Documentation\n\nDask Array documentation\n\nDask Array API\n\nDask Array examples","type":"content","url":"/notebooks/dask-array#in-this-tutorial-you-learn","position":5},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Dask Arrays"},"type":"lvl2","url":"/notebooks/dask-array#dask-arrays","position":6},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Dask Arrays"},"content":"Dask Arrays are basically parallelized version of NumPy arrays for processing larger-than-memory data sets.\n\nImage credit: Dask Contributors\n\nDask Array can be used as a drop-in replacement for NumPy arrays, with a similar API and support for a subset of NumPy functions.\n\nDask effectively reduces the memory footprint of large array computations by dividing the arrays into smaller pieces (called chunks) that can fit into memory and stream the data from disk.\n\nDask Arrays are lazy: Unlike Numpy, operations on Dask arrays are not computed until you explicitly request them.\n\nLazy Evaluation: objects are evaluated just in time when the results are needed!\n\nLazy evaluation help us avoid having large pieces of memory resident on the workers and optimize the resource requirements.\n\nDask Arrays don’t directly hold any data. Instead, they provide a symbolic representation of the necessary computations to generate the data. We will explain this more below.\n\nLet’s start exploring Dask Arrays:\n\n","type":"content","url":"/notebooks/dask-array#dask-arrays","position":7},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Setup: Start a Dask Client"},"type":"lvl2","url":"/notebooks/dask-array#setup-start-a-dask-client","position":8},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Setup: Start a Dask Client"},"content":"We will talk in-depth about Dask Cluster and Dask Clients later in this tutorial. Here we just created a local cluster and attached a client to it.\n\nfrom dask.distributed import Client\n\nclient = Client()\nclient\n\n","type":"content","url":"/notebooks/dask-array#setup-start-a-dask-client","position":9},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Blocked Algorithms"},"type":"lvl2","url":"/notebooks/dask-array#blocked-algorithms","position":10},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Blocked Algorithms"},"content":"Dask Arrays use blocked algorithms to split large computations into smaller computations which operate on subsets of the data (called chunks).\n\nLet’s see what this means in an example:\n\nimport numpy as np\nimport dask.array as da\n\n# A 4x4 numpy array that goes from 1 to 16 \n\nnarr = np.array([\n        [ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12],\n        [13, 14, 15, 16]\n        ])\n\n# -- convert numpy array to dask array with 4 chunks\ndarr = da.from_array( narr,chunks=(2, 2))\n\nNow we can calculate the sum of this array using darr.sum() similar to numpy. But how is it different from numpy?\n\nWhen you take the sum of the Dask array, Dask first takes the sum of each chunk and only after each of those is completed, takes the sum of the results from each chunk.\n\nImage adapted from \n\nsaturncloud.io\n\n","type":"content","url":"/notebooks/dask-array#blocked-algorithms","position":11},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Task Graph"},"type":"lvl2","url":"/notebooks/dask-array#task-graph","position":12},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Task Graph"},"content":"The Dask Task Graph serves as a blueprint for executing the computations.\n\nThe Task Graph defines the (1) relationships between tasks, and (2) the order in which they should be executed.\n\nIn a task graph each node in the graph represents a task and lines represent the dependencies/relationships between tasks.\n\nWe can visualize the low-level task graph using .visualize() method.\n\ndarr.sum().visualize(rankdir=\"LR\")\n\nIt is generally good practice to look at the task graph before executing the computation. By looking at the task graph, you can learn about potential bottlenecks where parallelism is not possible.\n\nTIP: For big computations, low-level task graphs gets very confusing. An alternative that provides a more concise graph is using .dask.visualize().\n\n#darr.sum().dask.visualize()\n\n\n\nNow, let’s start with another example. Here we create a 2D array of ones using NumPy.\n\nshape = (10000,12000)\n\nones_np = np.ones(shape)\nones_np\n\nNow, let’s create the same array using Dask:\n\nones_da = da.ones(shape)\nones_da\n\nWe see a Dask Array representation of the data.\nThis is a symbolic representation; no data has actually been generated yet.\n\nAs we discussed previously, this mode of operation is called “lazy”.\n\nThis allows the user to build up a series of computations or tasks before being passed to the scheduler for execution.\n\n","type":"content","url":"/notebooks/dask-array#task-graph","position":13},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Chunks"},"type":"lvl2","url":"/notebooks/dask-array#chunks","position":14},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Chunks"},"content":"When checking the Dask array, the symbolic representation illustrates the concept of chunks. Dask arrays split the data into sub-arrays (or chunks) to optimize computation with large arrays.\n\n","type":"content","url":"/notebooks/dask-array#chunks","position":15},{"hierarchy":{"lvl1":"Dask Array","lvl3":"Chunking an array","lvl2":"Chunks"},"type":"lvl3","url":"/notebooks/dask-array#chunking-an-array","position":16},{"hierarchy":{"lvl1":"Dask Array","lvl3":"Chunking an array","lvl2":"Chunks"},"content":"The way that arrays are chunked can significantly affect total performance.\n\nFor specifying the chunking of an array, we use the chunks argument when creating our dask.array.\n\n⚠️ WARNING: Please note that chunks argument stands for chunk shape rather than “number of chunks”. For example, chunks=1 means that you will have several chunks with one element.\n\nThere are several ways to define chunks. For example:\n\nA uniform dimension size like 1000, meaning chunks of size 1000 in each dimension.\n\nA uniform chunk shape like (1000, 2000, 3000), meaning chunks of size 1000 in the first axis, 2000 in the second axis, and 3000 in the third.\n\nFully explicit sizes of all blocks for all dimensions, like ((1000, 1000, 500), (400, 400), (5, 5, 5, 5, 5))\n\nA dictionary specifying chunk size per dimension like {0: 1000, 1: 2000, 2: 3000}.\n\nLet’s recreate the above Dask array, but this time we will specify chunk sizes (a.k.a. shapes) using the argument chunks.\n\n# -- remember what the shape of our data array was\nshape\n\n# create a dask array with 6 chunks\nchunk_shape = (5000,4000)\nones_da = da.ones(shape,chunks=chunk_shape)\nones_da\n\nYou can see in the above dask array representation that we now have 6 chunks, each of shape (5000,4000) and size of ~ 160.0 MiB.\n\n","type":"content","url":"/notebooks/dask-array#chunking-an-array","position":17},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Performance Comparison"},"type":"lvl2","url":"/notebooks/dask-array#performance-comparison","position":18},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Performance Comparison"},"content":"\n\nTo compare the performance between a NumPy array and an equivalent Dask array, let’s calculate the mean.\n\n%%time\n# The %%time cell magic measures the execution time of the whole cell\nones_np.mean()\n\n%%time\n# Remember, we are not doing any computation here, just constructing our task graph\nmean_of_ones_da = ones_da.mean()\n\nRemember :\n\nDask doesn’t do anything until you tell it... It is lazy!\n\nSo far we have just constructed our task graph but no computations yet!\n\nNOTE: In order to generate the data, we need to call the .compute() method on the Dask Array to trigger our computation.\n\nLet’s run the .compute() method to see how this works:\n\n%%time\nmean_of_ones_da.compute()\n\n.compute() method convertes Dask Arrays to Numpy Arrays. Let’s check to see if this is true:\n\ntype (ones_da.compute())\n\nWARNING: Typically, when working with Dask arrays, we do not want to generate the data right away by calling .compute() on a large array.\n\nWe usually want to perform some computations that reduce the data size. For example, we might compute statistics like the mean or standard deviation.\n\nLet’s look at an example of taking the mean and visualize the task graph. Remember, that no actual computation is taking place until we call .compute().\n\nmean_of_ones_da = ones_da.mean()\nmean_of_ones_da.visualize(rankdir=\"LR\")\n\n\n\nWhat are the sizes of these arrays in memory?\n\nFirst, let’s define a function that returns array size in MiB.\n\nimport sys\n\n# Define function to display variable size in MiB\ndef var_size(in_var):\n    result = sys.getsizeof(in_var) / 1024/1024\n    return (result)\n\nprint(\"Shape of the numpy array : \", ones_np.shape) \nprint(\"Shape of the dask array  : \", ones_da.shape) \n\n# memory size of numpy array in MiB\nprint(f\"Memory size of numpy array in MB : {var_size(ones_np):.2f} MiB\")\n# memory size of dask array in MiB\nprint(f\"Memory size of dask array in MB  : {var_size(ones_da):.2f} MiB\")\n\nWhy memory size for the above Dask array is zero?\n\nRemember, this variable is only a graph representation of the full array which will be split across workers.\n\nHowever, Dask does give us ways to see the full size of the data (often much larger than your client machine can handle)!\n\nprint(\"Size of Dask dataset:  {:.2f} MiB\".format(ones_da.nbytes / 1024/1024))\n\n","type":"content","url":"/notebooks/dask-array#performance-comparison","position":19},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Larger Data"},"type":"lvl2","url":"/notebooks/dask-array#larger-data","position":20},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Larger Data"},"content":"The previous example illustrated how Dask works, but using Dask is not really necessary (nor advisable) for an array of size 915.53 MiB.Let’s try an example using bigger data and bigger calculations:\n\nbig_shape = (2000, 200, 2000)\n\n# -- this will make a big numpy array that might not fit on your machine\n#big_np = np.ones(big_shape)\n\nMake a similar Dask Array with similar shape but specifying the chunks size:\n\nbig_da = da.ones(big_shape)\nbig_da\n\n# size of data\n#print(\"Memory size of NumPy dataset :  {:.2f} GiB\".format(big_np.nbytes / 1024/1024/1024))\nprint(\"Memory size of Dask dataset  :  {:.2f} GiB\".format(big_da.nbytes / 1024/1024/1024))\n\nThis may be close to the available memory/RAM that you have in your computer.\n\nLet’s try bigger calculations on this array:\n\n#%%time \n#z_np = (big_np + big_np.T)[::2,:].mean()\n\n%%time\n\nz_da = (big_da + big_da.T)[::2,:].mean(axis=2)\n\nresult = z_da.compute()\n\n#-- warning : do not try low level visualization with big arrays\n\n#z_da.visualize()\n\nAll the usual NumPy functions work on dask arrays, though the computations will remain lazy until you either call .compute(), .load() or your want to plot the data.\n\nAs we discussed above, the way that Dask arrays are chunked can significantly affect the performance. In the remainder of this notebook, let’s do a similar calculation using a different chunks size.\n\nWe will learn more about best practices regarding chunk size later during the tutorial.","type":"content","url":"/notebooks/dask-array#larger-data","position":21},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Supplementary Material: Rechunking Arrays"},"type":"lvl2","url":"/notebooks/dask-array#supplementary-material-rechunking-arrays","position":22},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Supplementary Material: Rechunking Arrays"},"content":"We can change the chunking of a Dask array, using the rechunk method. Please note that rechunking Dask arrays can be very expensive, so choosing an appropriate chunk size initially is ideal.\n\nnew_chunk_shape = (50,50,50)\nrechunked_big_da = big_da.rechunk(new_chunk_shape)\nrechunked_big_da\n\n%%time\n# perform big computation on chunked array\n\nz_da_rechunked = (rechunked_big_da + rechunked_big_da.T)[::2,:].mean(axis=2)\n\nresult = z_da_rechunked.compute()\n\nWe can see how the choice of smaller chunks (more total chunks) significantly reduce the total performance of our computation.\n\nTIP: As a rule of thumb, a chunk should be big enough so that the computation on that chunk take significantly longer than the overhead from Dask scheduler. The Dask scheduler takes roughly 1ms per task for scheduling.\n\nLet’s try a bigger chunk size:\n\nnew_chunk_shape = (500, 100, 500)\n#big_chunk = \n\nrechunked_big_da = big_da.rechunk(new_chunk_shape)\nrechunked_big_da\n\n%%time\n# perform big computation on chunked array\n\nz_da_rechunked = (rechunked_big_da + rechunked_big_da.T)[::2,:].mean(axis=2)\n\nresult = z_da_rechunked.compute()\n\nTIP: As a rule of thumb, a chunk should be small enough to fit comfortably in the memory. Chunk sizes between 10MB-1GB are common, depending on your machine,\n\nclient.close()\n\n","type":"content","url":"/notebooks/dask-array#supplementary-material-rechunking-arrays","position":23},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Summary:"},"type":"lvl2","url":"/notebooks/dask-array#summary","position":24},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Summary:"},"content":"Dask Array provides parallel computing capabilities by dividing arrays into smaller pieces called chunks.\n\nBlocked algorithms split large computations into smaller computations which operate on subsets of the array.\n\nDask Array supports efficient computation on large arrays through a combination of lazy evaluation and task parallelism.\n\nDask Array can be used as a drop-in replacement for NumPy ndarray, with a similar API and support for a subset of NumPy functions.\n\nThe way that arrays are chunked can significantly affect total performance. Poor chunking can singifincantly worsen performance of Dask compared to NumPy.\n\n","type":"content","url":"/notebooks/dask-array#summary","position":25},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/dask-array#resources-and-references","position":26},{"hierarchy":{"lvl1":"Dask Array","lvl2":"Resources and references"},"content":"Reference\n\nDask Docs\n\nDask Examples\n\nDask Code\n\nDask Blog\n\nAsk for help\n\ndask tag on Stack Overflow, for usage questions\n\ngithub discussions: dask for general, non-bug, discussion, and usage questions\n\ngithub issues: dask for bug reports and feature requests\n\nPieces of this notebook are adapted from the following sources\n\nDask Performace Comparison\n\nDask Arrays by EEDS","type":"content","url":"/notebooks/dask-array#resources-and-references","position":27},{"hierarchy":{"lvl1":"Dask DataFrame"},"type":"lvl1","url":"/notebooks/dask-dataframe","position":0},{"hierarchy":{"lvl1":"Dask DataFrame"},"content":"","type":"content","url":"/notebooks/dask-dataframe","position":1},{"hierarchy":{"lvl1":"Dask DataFrame"},"type":"lvl1","url":"/notebooks/dask-dataframe#dask-dataframe","position":2},{"hierarchy":{"lvl1":"Dask DataFrame"},"content":"","type":"content","url":"/notebooks/dask-dataframe#dask-dataframe","position":3},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"In this tutorial, you learn:"},"type":"lvl3","url":"/notebooks/dask-dataframe#in-this-tutorial-you-learn","position":4},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"In this tutorial, you learn:"},"content":"Basic concepts and features of Dask DataFrames\n\nApplications of Dask DataFrames\n\nInteracting with Dask DataFrames\n\nBuilt-in operations with Dask DataFrames\n\nDask DataFrames Best Practices","type":"content","url":"/notebooks/dask-dataframe#in-this-tutorial-you-learn","position":5},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Related Documentation"},"type":"lvl3","url":"/notebooks/dask-dataframe#related-documentation","position":6},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Related Documentation"},"content":"Dask DataFrame documentation\n\nDask DataFrame API\n\nDask DataFrame examples\n\npandas documentation","type":"content","url":"/notebooks/dask-dataframe#related-documentation","position":7},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/dask-dataframe#prerequisites","position":8},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nFamiliarity with Pandas DataFrame\n\nNecessary\n\n\n\nDask Overview\n\nNecessary\n\n\n\nTime to learn: 40 minutes","type":"content","url":"/notebooks/dask-dataframe#prerequisites","position":9},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Introduction"},"type":"lvl2","url":"/notebooks/dask-dataframe#introduction","position":10},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Introduction"},"content":"\n\nImage credit: Dask Contributors\n\npandas is a very popular tool for working with tabular datasets, but the dataset needs to fit into the memory.\n\npandas operates best with smaller datasets, and if you have a large dataset, you’ll receive an out of memory error using pandas. A general rule of thumb for pandas is:\n\n“Have 5 to 10 times as much RAM as the size of your dataset”\n\nWes McKinney (2017) in \n\n10 things I hate about pandas\n\nBut Dask DataFrame can be used to solve pandas performance issues with larger-than-memory datasets.","type":"content","url":"/notebooks/dask-dataframe#introduction","position":11},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"What is Dask DataFrame?","lvl2":"Introduction"},"type":"lvl3","url":"/notebooks/dask-dataframe#what-is-dask-dataframe","position":12},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"What is Dask DataFrame?","lvl2":"Introduction"},"content":"A Dask DataFrame is a parallel DataFrame composed of smaller pandas DataFrames (also known as partitions).\n\nDask Dataframes look and feel like the pandas DataFrames on the surface.\n\nDask DataFrames partition the data into manageable partitions that can be processed in parallel and across multiple cores or computers.\n\nSimilar to Dask Arrays, Dask DataFrames are lazy!\n\nUnlike pandas, operations on Dask DataFrames are not computed until you explicitly request them (e.g. by calling .compute).\n\n","type":"content","url":"/notebooks/dask-dataframe#what-is-dask-dataframe","position":13},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"When to use Dask DataFrame and when to avoid it?"},"type":"lvl2","url":"/notebooks/dask-dataframe#when-to-use-dask-dataframe-and-when-to-avoid-it","position":14},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"When to use Dask DataFrame and when to avoid it?"},"content":"Dask DataFrames are used in situations where pandas fails or has poor performance due to data size.\n\nDask DataFrame is a good choice when doing parallalizeable computations.Some examples are:\n\nElement-wise operations such as df.x + df.y\n\nRow-wise filtering such as df[df.x>0]\n\nCommon aggregations such as df.x.max()\n\nDropping duplicates such as df.x.drop_duplicate()\n\nHowever, Dask is not great for operations that requires shuffling or re-indexing.Some examples are:\n\nSet index: df.set_index(df.x)\n\nWARNING: Although, Dask DataFrame has a very similar interface to the pandas DataFrame (as we will see in this tutorial), it does NOT include some of the pandas interface yet.\n\nSee the \n\nDask DataFrame API documentation for a compehnsive list of available functions.\n\n","type":"content","url":"/notebooks/dask-dataframe#when-to-use-dask-dataframe-and-when-to-avoid-it","position":15},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Tutorial Dataset"},"type":"lvl2","url":"/notebooks/dask-dataframe#tutorial-dataset","position":16},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Tutorial Dataset"},"content":"In this tutorial, we are going to use the NOAA Global Historical Climatology Network Daily (GHCN-D) dataset.GHCN-D is a public available dataset that includes daily climate records from +100,000 surface observations around the world.This is an example of a real dataset that is used by NCAR scientists for their research. GHCN-D raw dataset for all stations is available through \n\nNOAA Climate Data Online.\n\nTo learn more about GHCNd dataset, please visit:\n\nGHCNd Journal Paper\n\nGHCNd Official Website","type":"content","url":"/notebooks/dask-dataframe#tutorial-dataset","position":17},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Download the data","lvl2":"Tutorial Dataset"},"type":"lvl3","url":"/notebooks/dask-dataframe#download-the-data","position":18},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Download the data","lvl2":"Tutorial Dataset"},"content":"For this example, we are going to look through a subset of data from the GHCN-D dataset.\n\nFirst, we look at the daily observations from Denver International Airport, next we are going to look through selected stations in the US.\n\nThe access the preprocessed dataset for this tutorial, please run the following script:\n\n!./get_data.sh\n\nThis script should save the preprocessed GHCN-D data in ../data path.\n\n","type":"content","url":"/notebooks/dask-dataframe#download-the-data","position":19},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Pandas DataFrame Basics"},"type":"lvl2","url":"/notebooks/dask-dataframe#pandas-dataframe-basics","position":20},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Pandas DataFrame Basics"},"content":"Let’s start with an example using pandas DataFrame.\n\nFirst, let’s read in the comma-seperated GHCN-D dataset for one station at Denver International Airport (DIA), CO (site ID : USW00003017).\n\nTo see the list of all available GHCN-D sites and their coordinates and IDs, please see \n\nthis link.\n\nimport os\nimport pandas as pd\n\n# DIA ghcnd id\nsite = 'USW00003017'\ndata_dir = '../data/'\n\n\ndf = pd.read_csv(os.path.join(data_dir, site+'.csv'), parse_dates=['DATE'], index_col=0)\n\n# Display the top five rows of the dataframe\ndf.head()\n\nQuestion: What variables are available?\n\ndf.columns\n\nThe description and units of the dataset is available \n\nhere.\n\n","type":"content","url":"/notebooks/dask-dataframe#pandas-dataframe-basics","position":21},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Operations on pandas DataFrame","lvl2":"Pandas DataFrame Basics"},"type":"lvl3","url":"/notebooks/dask-dataframe#operations-on-pandas-dataframe","position":22},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Operations on pandas DataFrame","lvl2":"Pandas DataFrame Basics"},"content":"pandas DataFrames has several features that give us flexibility to do different calculations and analysis on our dataset. Let’s check some out:","type":"content","url":"/notebooks/dask-dataframe#operations-on-pandas-dataframe","position":23},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"Simple Analysis","lvl3":"Operations on pandas DataFrame","lvl2":"Pandas DataFrame Basics"},"type":"lvl4","url":"/notebooks/dask-dataframe#simple-analysis","position":24},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"Simple Analysis","lvl3":"Operations on pandas DataFrame","lvl2":"Pandas DataFrame Basics"},"content":"\n\nFor example:\n\nWhen was the coldest day at this station during December of last year?\n\n# use python slicing notation inside .loc \n# use idxmin() to find the index of minimum valus\ndf.loc['2022-12-01':'2022-12-31'].TMIN.idxmin()\n\n# Here we easily plot the prior data using matplotlib from pandas\n# -- .loc for value based indexing\ndf.loc['2022-12-01':'2022-12-31'].SNWD.plot(ylabel= 'Daily Average Snow Depth [mm]')\n\nHow many snow days do we have each year at this station?\n\nPandas groupby is used for grouping the data according to the categories.\n\n# 1- First select days with snow > 0\n# 2- Create a \"groupby object\" based on the selected columns\n# 3- use .size() to compute the size of each group\n# 4- sort the values descending \n\n# we count days where SNOW>0, and sort them and show top 5 years:\ndf[df['SNOW']>0].groupby('YEAR').size().sort_values(ascending=False).head()\n\n\nOr for a more complex analysis:\n\nFor example, we have heard that this could be Denver’s first January in 13 years with no 60-degree days.\n\n\n\nBelow, we show all days with high temperature above 60°F (155.5°C/10) since 2010:\n\ndf[(df['MONTH']==1) & (df['YEAR']>=2010) & (df['TMAX']>155.5)].groupby(['YEAR']).size()\n\nThis is great! But how big is this dataset for one station?\n\nFirst, let’s check the file size:\n\n!ls -lh ../data/USW00003017.csv\n\nSimilar to the previous tutorial, we can use the following function to find the size of a variable on memory.\n\n# Define function to display variable size in MB\nimport sys\ndef var_size(in_var):\n    result = sys.getsizeof(in_var) / 1e6\n    print(f\"Size of variable: {result:.2f} MB\")\n\nvar_size(df)\n\nRemember, the above rule?\n\n“Have 5 to 10 times as much RAM as the size of your dataset”\n\nWes McKinney (2017) in \n\n10 things I hate about pandas\n\nSo far, we read in and analyzed data for one station. We have a total of +118,000 stations over the world and +4500 stations in Colorado alone!\n\nWhat if we want to look at the larger dataset?","type":"content","url":"/notebooks/dask-dataframe#simple-analysis","position":25},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Scaling up to a larger dataset"},"type":"lvl2","url":"/notebooks/dask-dataframe#scaling-up-to-a-larger-dataset","position":26},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Scaling up to a larger dataset"},"content":"Let’s start by reading data from selected stations. The downloaded data for this example includes the climatology observations from 66 selected sites in Colorado.\n\nPandas can concatenate data to load data spread across multiple files:\n\n!du -csh ../data/*.csv |tail -n1\n\nUsing a for loop with pandas.concat, we can read multiple files at the same time:\n\n%%time\nimport glob\nco_sites = glob.glob(os.path.join(data_dir, '*.csv'))\ndf = pd.concat(pd.read_csv(f, index_col=0, parse_dates=['DATE']) for f in co_sites)\n\nHow many stations have we read in?\n\nprint (\"Concatenated data for\", len(df.ID.unique()), \"unique sites.\")\n\nNow that we concatenated the data for all sites in one DataFrame, we can do similar analysis on it:\n\nWhich site has recorded the most snow days in a year?\n\n%%time\n# ~90s on 4GB RAM\nsnowy_days = df[df['SNOW']>0].groupby(['ID','YEAR']).size()\n\nprint ('This site has the highest number of snow days in a year : ')\nsnowy_days.agg(['idxmax','max'])\n\nExcersise: Which Colorado site has recorded the most snow days in 2023?\n\nDask allows us to conceptualize all of these files as a single dataframe!\n\n# Let's do a little cleanup\ndel df, snowy_days\n\n","type":"content","url":"/notebooks/dask-dataframe#scaling-up-to-a-larger-dataset","position":27},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Computations on Dask DataFrame"},"type":"lvl2","url":"/notebooks/dask-dataframe#computations-on-dask-dataframe","position":28},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Computations on Dask DataFrame"},"content":"\n\n","type":"content","url":"/notebooks/dask-dataframe#computations-on-dask-dataframe","position":29},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Create a “LocalCluster” Client with Dask","lvl2":"Computations on Dask DataFrame"},"type":"lvl3","url":"/notebooks/dask-dataframe#create-a-localcluster-client-with-dask","position":30},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Create a “LocalCluster” Client with Dask","lvl2":"Computations on Dask DataFrame"},"content":"\n\nfrom dask.distributed import Client, LocalCluster\n\ncluster = LocalCluster()\nclient = Client(cluster)\nclient\n\n☝️ Click the Dashboard link above.\n\n👈 Or click the “Search” 🔍 button in the dask-labextension dashboard.","type":"content","url":"/notebooks/dask-dataframe#create-a-localcluster-client-with-dask","position":31},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Dask DataFrame read_csv to read multiple files","lvl2":"Computations on Dask DataFrame"},"type":"lvl3","url":"/notebooks/dask-dataframe#dask-dataframe-read-csv-to-read-multiple-files","position":32},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Dask DataFrame read_csv to read multiple files","lvl2":"Computations on Dask DataFrame"},"content":"dask.dataframe.read_csv function can be used in conjunction with glob to read multiple csv files at the same time.\n\nRemember we can read one file with pandas.read_csv. For reading multiple files with pandas, we have to concatenate them with pd.concatenate. However, we can read many files at once just using dask.dataframe.read_csv.\n\nOverall, Dask is designed to perform I/O in parallel and is more performant than pandas for operations with multiple files or large files.\n\n%%time\nimport dask\nimport dask.dataframe as dd\n\nddf = dd.read_csv(co_sites, parse_dates=['DATE'])\nddf\n\nddf.TMAX.mean()\n\nNotice that the representation of the DataFrame object contains no data just headers and datatypes.  Why?","type":"content","url":"/notebooks/dask-dataframe#dask-dataframe-read-csv-to-read-multiple-files","position":33},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"Lazy Evaluation","lvl3":"Dask DataFrame read_csv to read multiple files","lvl2":"Computations on Dask DataFrame"},"type":"lvl4","url":"/notebooks/dask-dataframe#lazy-evaluation","position":34},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"Lazy Evaluation","lvl3":"Dask DataFrame read_csv to read multiple files","lvl2":"Computations on Dask DataFrame"},"content":"Similar to Dask Arrays, Dask DataFrames are lazy. Here the data has not yet been read into the dataframe yet (a.k.a. lazy evaluation).Dask just construct the task graph of the computation but it will “evaluate” them only when necessary.\n\nSo how does Dask know the name and dtype of each column?\n\nDask has just read the start of the first file and infers the column names and dtypes.\n\nUnlike pandas.read_csv that reads in all files before inferring data types, dask.dataframe.read_csv only reads in a sample from the beginning of the file (or first file if using a glob). The column names and dtypes are then enforced when reading the specific partitions (Dask can make mistakes on these inferences if there is missing or misleading data in the early rows).\n\nLet’s take a look at the start of our dataframe:\n\nddf.head()\n\nNOTE: Whenever we operate on our dataframe we read through all of our CSV data so that we don’t fill up RAM. Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible. This enables you to handle larger than memory datasets but, repeated computations will have to load all of the data in each time.\n\nSimilar data manipulations as pandas.dataframe can be done for dask.dataframes.For example, let’s find the highest number of snow days in Colorado:\n\n%%time\nprint ('This site has the highest number of snow days in a year : ')\nsnowy_days = ddf[ddf['SNOW']>0].groupby(['ID','YEAR']).size()\nsnowy_days.compute().agg(['idxmax','max'])\n\n","type":"content","url":"/notebooks/dask-dataframe#lazy-evaluation","position":35},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"Nice, but what did Dask do?","lvl3":"Dask DataFrame read_csv to read multiple files","lvl2":"Computations on Dask DataFrame"},"type":"lvl4","url":"/notebooks/dask-dataframe#nice-but-what-did-dask-do","position":36},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"Nice, but what did Dask do?","lvl3":"Dask DataFrame read_csv to read multiple files","lvl2":"Computations on Dask DataFrame"},"content":"\n\n# Requires ipywidgets\n\nsnowy_days.dask\n\nYou can also view the underlying task graph using .visualize():\n\n#graph is too large\nsnowy_days.visualize()\n\n","type":"content","url":"/notebooks/dask-dataframe#nice-but-what-did-dask-do","position":37},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Use .compute wisely!","lvl2":"Computations on Dask DataFrame"},"type":"lvl3","url":"/notebooks/dask-dataframe#use-compute-wisely","position":38},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Use .compute wisely!","lvl2":"Computations on Dask DataFrame"},"content":"","type":"content","url":"/notebooks/dask-dataframe#use-compute-wisely","position":39},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"Share intermediate results","lvl3":"Use .compute wisely!","lvl2":"Computations on Dask DataFrame"},"type":"lvl4","url":"/notebooks/dask-dataframe#share-intermediate-results","position":40},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"Share intermediate results","lvl3":"Use .compute wisely!","lvl2":"Computations on Dask DataFrame"},"content":"\n\nFor most operations, dask.dataframe hashes the arguments, allowing duplicate computations to be shared, and only computed once.\n\nFor example, let’s compute the mean and standard deviation for Maximum daily temperature of all snow days.\n\nsnowy_days = ddf[ddf['SNOW']>0]\nmean_tmax = snowy_days.TMAX.mean()\nstd_tmax = snowy_days.TMAX.std()\n\n%%time\n\nmean_tmax_result = mean_tmax.compute()\nstd_tmax_result = std_tmax.compute()\n\nBut if we pass both arguments in a single .compute, we can share the intermediate results:\n\n%%time\nmean_tmax_result, std_tmax_result = dask.compute(mean_tmax, std_tmax)\n\nHere using dask.compute only one allowed sharing intermediate results between TMAX mean and median calculations and improved total performance.\n\nmean_tmax.dask\n\nHere some operations such as the calls to read the csv files, the filtering, and the grouping is exactly similar between both operations, so they can share intermediate results. Remember, Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible.\n\n","type":"content","url":"/notebooks/dask-dataframe#share-intermediate-results","position":41},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":".persist or caching","lvl2":"Computations on Dask DataFrame"},"type":"lvl3","url":"/notebooks/dask-dataframe#id-persist-or-caching","position":42},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":".persist or caching","lvl2":"Computations on Dask DataFrame"},"content":"Sometimes you might want your computers to keep intermediate results in memory, if it fits in the memory.\n\nThe .persist() method can be used to  “cache” data and tell Dask what results to keep around. You should only use .persist() with any data or computation that fits in memory.\n\nFor example, if we want to only do analysis on a subset of data (for example snow days at Boulder site):\n\nboulder_snow = ddf[(ddf['SNOW']>0)&(ddf['ID']=='USC00050848')]\n\n%%time\ntmax = boulder_snow.TMAX.mean().compute()\ntmin = boulder_snow.TMIN.mean().compute()\n\nprint (tmin, tmax)\n\nboulder_snow = ddf[(ddf['SNOW']>0)&(ddf['ID']=='USC00050848')].persist()\n\n%%time\n\ntmax = boulder_snow.TMAX.mean().compute()\ntmin = boulder_snow.TMIN.mean().compute()\nprint (tmin, tmax)\n\nAs you can see the analysis on this persisted data is much faster because we are not repeating the loading and selecting.\n\n","type":"content","url":"/notebooks/dask-dataframe#id-persist-or-caching","position":43},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Dask DataFrames Best Practices"},"type":"lvl2","url":"/notebooks/dask-dataframe#dask-dataframes-best-practices","position":44},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Dask DataFrames Best Practices"},"content":"","type":"content","url":"/notebooks/dask-dataframe#dask-dataframes-best-practices","position":45},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Use pandas (when you can)","lvl2":"Dask DataFrames Best Practices"},"type":"lvl3","url":"/notebooks/dask-dataframe#use-pandas-when-you-can","position":46},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Use pandas (when you can)","lvl2":"Dask DataFrames Best Practices"},"content":"For data that fits into RAM, pandas can often be easier and more efficient to use than Dask DataFrame. However, Dask DataFrame is a powerful tool for larger-than-memory datasets.\n\nWhen the data is still larger than memory, Dask DataFrame can be used to reduce the larger datasets to a manageable level that pandas can handle. Next, use pandas at that point.","type":"content","url":"/notebooks/dask-dataframe#use-pandas-when-you-can","position":47},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Avoid Full-Data Shuffling","lvl2":"Dask DataFrames Best Practices"},"type":"lvl3","url":"/notebooks/dask-dataframe#avoid-full-data-shuffling","position":48},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Avoid Full-Data Shuffling","lvl2":"Dask DataFrames Best Practices"},"content":"Some operations are more expensive to compute in a parallel setting than if they are in-memory on a single machine (for example, set_index or merge). In particular, shuffling operations that rearrange data can become very communication intensive.","type":"content","url":"/notebooks/dask-dataframe#avoid-full-data-shuffling","position":49},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"pandas performance tips","lvl2":"Dask DataFrames Best Practices"},"type":"lvl3","url":"/notebooks/dask-dataframe#pandas-performance-tips","position":50},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"pandas performance tips","lvl2":"Dask DataFrames Best Practices"},"content":"pandas performance tips such as using vectorized operations also apply to Dask DataFrames. See \n\nModern Pandas notebook for more tips on better performance with pandas.","type":"content","url":"/notebooks/dask-dataframe#pandas-performance-tips","position":51},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Check Partition Size","lvl2":"Dask DataFrames Best Practices"},"type":"lvl3","url":"/notebooks/dask-dataframe#check-partition-size","position":52},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Check Partition Size","lvl2":"Dask DataFrames Best Practices"},"content":"Similar to chunks, partitions should be small enough that they fit in the memory, but large enough to avoid that the communication overhead.","type":"content","url":"/notebooks/dask-dataframe#check-partition-size","position":53},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"blocksize","lvl3":"Check Partition Size","lvl2":"Dask DataFrames Best Practices"},"type":"lvl4","url":"/notebooks/dask-dataframe#blocksize","position":54},{"hierarchy":{"lvl1":"Dask DataFrame","lvl4":"blocksize","lvl3":"Check Partition Size","lvl2":"Dask DataFrames Best Practices"},"content":"The number of partitions can be set using the blocksize argument.\nIf none is given, the number of partitions/blocksize is calculated depending on the available memory and the number of cores on a machine up to a max of 64 MB. As we increase the blocksize, the number of partitions (calculated by Dask) will decrease. This is especially important when reading one large csv file.\n\nAs a good rule of thumb, you should aim for partitions that have around 100MB of data each.","type":"content","url":"/notebooks/dask-dataframe#blocksize","position":55},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Smart use of .compute()","lvl2":"Dask DataFrames Best Practices"},"type":"lvl3","url":"/notebooks/dask-dataframe#smart-use-of-compute","position":56},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Smart use of .compute()","lvl2":"Dask DataFrames Best Practices"},"content":"Try avoiding running .compute() operation as long as possible. Dask works best when users avoid computation until results are needed. The .compute() command informs Dask to trigger computations on the Dask DataFrame.As shown in the above example, the intermediate results can also be shared by calling .compute() only once.\n\n","type":"content","url":"/notebooks/dask-dataframe#smart-use-of-compute","position":57},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Close your local Dask Cluster","lvl2":"Dask DataFrames Best Practices"},"type":"lvl3","url":"/notebooks/dask-dataframe#close-your-local-dask-cluster","position":58},{"hierarchy":{"lvl1":"Dask DataFrame","lvl3":"Close your local Dask Cluster","lvl2":"Dask DataFrames Best Practices"},"content":"It is always a good practice to close the Dask cluster you created.\n\nclient.shutdown()\n\n\n\n","type":"content","url":"/notebooks/dask-dataframe#close-your-local-dask-cluster","position":59},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/dask-dataframe#summary","position":60},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Summary"},"content":"In this notebook, we have learned about:\n\nDask DataFrame concept and component.\n\nWhen to use and when to avoid Dask DataFrames?\n\nHow to use Dask DataFrame?\n\nSome best practices around Dask DataFrames.","type":"content","url":"/notebooks/dask-dataframe#summary","position":61},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/dask-dataframe#resources-and-references","position":62},{"hierarchy":{"lvl1":"Dask DataFrame","lvl2":"Resources and references"},"content":"Reference\n\nDask Docs\n\nDask Examples\n\nDask Code\n\nDask Blog\n\nPandas Docs\n\nAsk for help\n\ndask tag on Stack Overflow, for usage questions\n\ngithub discussions: dask for general, non-bug, discussion, and usage questions\n\ngithub issues: dask for bug reports and feature requests","type":"content","url":"/notebooks/dask-dataframe#resources-and-references","position":63},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask"},"type":"lvl1","url":"/notebooks/dask-xarray","position":0},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask"},"content":"\n","type":"content","url":"/notebooks/dask-xarray","position":1},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask"},"type":"lvl1","url":"/notebooks/dask-xarray#parallelizing-xarray-with-dask","position":2},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask"},"content":"","type":"content","url":"/notebooks/dask-xarray#parallelizing-xarray-with-dask","position":3},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"In this tutorial, you learn:"},"type":"lvl3","url":"/notebooks/dask-xarray#in-this-tutorial-you-learn","position":4},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"In this tutorial, you learn:"},"content":"Using Dask with Xarray\n\nRead/write netCDF files with Dask\n\nDask backed Xarray objects and operations\n\nExtract Dask arrays from Xarray objects and use Dask array directly.\n\nXarray built-in operations can transparently use dask","type":"content","url":"/notebooks/dask-xarray#in-this-tutorial-you-learn","position":5},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/dask-xarray#prerequisites","position":6},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nDask Arrays\n\nNecessary\n\n\n\nDask DataFrames\n\nNecessary\n\n\n\nTime to learn: 40 minutes","type":"content","url":"/notebooks/dask-xarray#prerequisites","position":7},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Introduction"},"type":"lvl2","url":"/notebooks/dask-xarray#introduction","position":8},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Introduction"},"content":"","type":"content","url":"/notebooks/dask-xarray#introduction","position":9},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Xarray Quick Overview","lvl2":"Introduction"},"type":"lvl3","url":"/notebooks/dask-xarray#xarray-quick-overview","position":10},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Xarray Quick Overview","lvl2":"Introduction"},"content":"Xarray is an open-source Python library designed for working with labelled multi-dimensional data. By multi-dimensional data (also often called N-dimensional), we mean data that has many independent dimensions or axes (e.g. latitude, longitude, time). By labelled we mean that these axes or dimensions are associated with coordinate names (like “latitude”) and coordinate labels like “30 degrees North”.\n\nXarray provides pandas-level convenience for working with this type of data.\n\n\n\nImage credit: Xarray Contributors\n\nThe dataset illustrated has two variables (temperature and precipitation) that have three dimensions. Coordinate vectors (e.g., latitude, longitude, time) that describe the data are also included.","type":"content","url":"/notebooks/dask-xarray#xarray-quick-overview","position":11},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl4":"Xarray Data Structures","lvl3":"Xarray Quick Overview","lvl2":"Introduction"},"type":"lvl4","url":"/notebooks/dask-xarray#xarray-data-structures","position":12},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl4":"Xarray Data Structures","lvl3":"Xarray Quick Overview","lvl2":"Introduction"},"content":"Xarray has two fundamental data structures:\n\nDataArray : holds a single multi-dimensional variable and its coordinates\n\nDataset : holds multiple DataArrays that potentially share the same coordinates\n\nXarray DataArray\n\nA DataArray has four essential attributes:\n\ndata: a numpy.ndarray holding the values.\n\ndims: dimension names for each axis (e.g., latitude, longitude, time).\n\ncoords: a dict-like container of arrays (coordinates) that label each point (e.g., 1-dimensional arrays of numbers, datetime objects or strings).\n\nattrs: a dictionary to hold arbitrary metadata (attributes).\n\nXarray DataSet\n\nA dataset is simply an object containing multiple Xarray DataArrays indexed by variable name.\n\n","type":"content","url":"/notebooks/dask-xarray#xarray-data-structures","position":13},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Xarray can wrap many array types like Numpy and Dask.","lvl2":"Introduction"},"type":"lvl3","url":"/notebooks/dask-xarray#xarray-can-wrap-many-array-types-like-numpy-and-dask","position":14},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Xarray can wrap many array types like Numpy and Dask.","lvl2":"Introduction"},"content":"Let’s start with a random 2D NumPy array, for example this can be SST (sea-surface temperature) values of a domain with dimension of 300x450 grid:\n\nimport numpy as np \nimport dask.array as da\nimport xarray as xr\n\nxr.set_options(display_expand_data=False);\n\n# -- numpy array \nsst_np = np.random.rand(300,450)\ntype(sst_np)\n\nAs we saw in the previous tutorial, we can convert them to a Dask Array:\n\nsst_da = da.from_array( sst_np)\nsst_da\n\nThis is great and fast! BUT\n\nWhat if we want to attach coordinate values to this array?\n\nWhat if we want to add metadata (e.g. units) to this array?\n\n# similarly we can convert them to xarray datarray\nsst_xr = xr.DataArray(sst_da)\nsst_xr\n\nA simple DataArray without dimensions or coordinates isn’t much use.\n\n# we can add dimension names to this:\nsst_xr = xr.DataArray(sst_da,dims=['lat','lon'])\n\nsst_xr.dims\n\nWe can add our coordinates with values to it :\n\n# -- create some dummy values for lat and lon dimensions\nlat = np.random.uniform(low=-90, high=90, size=300)\nlon = np.random.uniform(low=-180, high=180, size=450)\n\nsst_xr = xr.DataArray(sst_da,\n                      dims=['lat','lon'],\n                      coords={'lat': lat, 'lon':lon},\n                      attrs=dict(\n                        description=\"Sea Surface Temperature.\",\n                        units=\"degC\")\n                     )\nsst_xr\n\nXarray data structures are a very powerful tool that allows us to use metadata to express different analysis patterns (slicing, selecting, groupby, averaging, and many other things).\n\nTake Away\n\nXarray DataArray provides a wrapper around arrays, and uses labeled dimensions and coordinates to support metadata-aware operations (e.g. da.sum(dim=\"time\") instead of array.sum(axis=-1))\n\nXarray can wrap dask arrays instead of numpy arrays.\n\nThis capability turns Xarray into an extremely useful tool for Big Data earth science.\n\nWith this introduction, let’s start our tutorial on features of Xarray and Dask:\n\n","type":"content","url":"/notebooks/dask-xarray#xarray-can-wrap-many-array-types-like-numpy-and-dask","position":15},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Setup: Spinning up a cluster","lvl2":"Introduction"},"type":"lvl3","url":"/notebooks/dask-xarray#setup-spinning-up-a-cluster","position":16},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Setup: Spinning up a cluster","lvl2":"Introduction"},"content":"\n\nfrom dask.distributed import LocalCluster, Client\ncluster = LocalCluster()\nclient = Client(cluster)\nclient\n\n\n","type":"content","url":"/notebooks/dask-xarray#setup-spinning-up-a-cluster","position":17},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Reading data with Dask and Xarray"},"type":"lvl2","url":"/notebooks/dask-xarray#reading-data-with-dask-and-xarray","position":18},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Reading data with Dask and Xarray"},"content":"","type":"content","url":"/notebooks/dask-xarray#reading-data-with-dask-and-xarray","position":19},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Reading multiple netCDF files with open_mfdataset","lvl2":"Reading data with Dask and Xarray"},"type":"lvl3","url":"/notebooks/dask-xarray#reading-multiple-netcdf-files-with-open-mfdataset","position":20},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Reading multiple netCDF files with open_mfdataset","lvl2":"Reading data with Dask and Xarray"},"content":"Xarray provides a function called open_dataset function that allows us to load a netCDF dataset into a Python data structure. To read more about this function, please see \n\nxarray open_dataset API documentation.\n\nXarray also provides open_mfdataset, which open multiple files as a single xarray dataset. Passing the argument parallel=True will speed up reading multiple datasets by executing these tasks in parallel using Dask Delayed under the hood.\n\nIn this example, we are going to examine a subset  of CESM2 Large Ensemble Data Sets (LENS). We will use 2m temperature (TREFHT) for this analysis.\n\nTo learn more about LENS dataset, please visit:\n\nLENS official website\n\nLENS paper\n\nFor this tutorial, we only look at a small subset of data. If you don’t have the data, running the following code enables you to download, prepare, and stage the required datasets (../data/ folder) for this cookbook.\n\n!./get_data.sh notebook3\n\nWe can open up multiple files using open_mfdataset function.\n\n","type":"content","url":"/notebooks/dask-xarray#reading-multiple-netcdf-files-with-open-mfdataset","position":21},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Constructing Xarray Datasets from files","lvl2":"Reading data with Dask and Xarray"},"type":"lvl3","url":"/notebooks/dask-xarray#constructing-xarray-datasets-from-files","position":22},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Constructing Xarray Datasets from files","lvl2":"Reading data with Dask and Xarray"},"content":"\n\nimport os\nimport glob\n\nvar = 'TREFHT'\n\n# find all LENS files for 1 ensemble\ndata_dir = '../data/data_for_cesm'\nfiles = glob.glob(os.path.join(data_dir, 'b.e21.BSSP370smbb.f09_g17.LE2-1301.013*.nc'))\n\nprint(\"All files: [\", len(files), \"files]\")\n\n%%time\nds = xr.open_mfdataset(\n    sorted(files),\n    # concatenate along this dimension\n    concat_dim=\"time\",\n    # concatenate files in the order provided\n    combine=\"nested\",\n    # parallelize the reading of individual files using dask\n    # This means the returned arrays will be dask arrays\n    parallel=True,\n    # these are netCDF4 files, use the h5netcdf package to read them\n    engine=\"h5netcdf\",\n    # hold off on decoding time\n    decode_cf=False,\n    # specify that data should be automatically chunked\n    chunks=\"auto\",\n)\nds = xr.decode_cf(ds)\nds\n\nFor complex scenarios, you can access each file individually by utilizing the open_dataset function with the specified chunks and then combine the outputs into a single dataset later.\n\nNote that the “real” values are not displayed, since that would trigger actual computation.\n\nXarray automatically wraps Dask Arrays and Dask is lazy, meaning that operations are not computed until we explicitly request them, for example by calling .compute().\n\nPlease see previous notebooks for more information on “lazy evaluation”.\n\nThe represntation of TREFHT DataArray shows details of chunks and chunk-sizes of Xarray DataArray:\n\ntref = ds.TREFHT\ntref\n\ntref.chunks\n\nHow many chunks do we have?\n\nWhat is the size of each chunk size?\n\nHere we can see that we have a total of 9 chunks - equal to the number of our netCDF files. In general open_mfdataset will return one chunk per netCDF file.\n\nWARNING: The chunk structure within the file is important. When re-chunking the dataset after creation with ds.chunk() it is recommended to only use multiples of the on-file chunk shape.\n\nWe can check what that shape is by looking at the encoding:\n\ntref.encoding\n\nTIP: The chunks parameter can significantly affect total performance when using Dask Arrays. chunks should be small enough that each chunk fit in the memory, but large enough to avoid that the communication overhead.\n\nA good rule of thumb is to create arrays with a minimum chunksize of at least one million elements. Here we have 120x192x288 elements in each chunk (except for the last chunk).With large arrays (10+ GB), the cost of queuing up Dask operations can be noticeable, and you may need even larger chunksizes.\n\nAdditional Reading\n\ndask.array best practices\n\nNCAR chunking tutorial\n\nDask blog post on chunking\n\n","type":"content","url":"/notebooks/dask-xarray#constructing-xarray-datasets-from-files","position":23},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Xarray data structures are Dask collections.","lvl2":"Reading data with Dask and Xarray"},"type":"lvl3","url":"/notebooks/dask-xarray#xarray-data-structures-are-dask-collections","position":24},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Xarray data structures are Dask collections.","lvl2":"Reading data with Dask and Xarray"},"content":"This means you can call the following Dask-related functions on Xarray Data Arrays and Datasets:\n\n.visualize()\n\n.compute()\n\n.persist()\n\nFor more information about Dask Arrays, please see \n\nDask Array chapter.\n\ntref_mean = tref.mean('time')\ntref_mean.data.dask\n\nIf we check Dask Task Graph for tref_mean, we can see all the steps required for calculating it (from opening the netcdf file to calculating mean and aggreagting it).\n\n","type":"content","url":"/notebooks/dask-xarray#xarray-data-structures-are-dask-collections","position":25},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Getting concrete values","lvl2":"Reading data with Dask and Xarray"},"type":"lvl3","url":"/notebooks/dask-xarray#getting-concrete-values","position":26},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Getting concrete values","lvl2":"Reading data with Dask and Xarray"},"content":"At some point, you will want to actually do the calculations and receive concrete values from Dask.\n\nThere are two ways to compute values on dask arrays.\n\ncompute() returns a new xarray object with the data now represented as a numpy array.\n\nload() replaces the dask array in the xarray object with a numpy array. Equivalent to ds = ds.compute().\n\n.load() operates inplace and .compute() returns a new xarray object.","type":"content","url":"/notebooks/dask-xarray#getting-concrete-values","position":27},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl4":"Distributed non-blocking concrete values","lvl3":"Getting concrete values","lvl2":"Reading data with Dask and Xarray"},"type":"lvl4","url":"/notebooks/dask-xarray#distributed-non-blocking-concrete-values","position":28},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl4":"Distributed non-blocking concrete values","lvl3":"Getting concrete values","lvl2":"Reading data with Dask and Xarray"},"content":"There is another option available  third option : “persisting”. .persist() loads the values into distributed RAM. The values are computed but remain distributed across workers. So essentially persist turns a lazy Dask collection into a Dask collection where the results are either fully computed or actively computing in the background.\n\nSo ds.air.persist() is still backed by a Dask array. This is useful if you will be repeatedly using a dataset for computation but it is too large to load into local memory.\n\nRead more: \n\nDask user guide\n\n","type":"content","url":"/notebooks/dask-xarray#distributed-non-blocking-concrete-values","position":29},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"How to access underlying data in an Xarray object?"},"type":"lvl2","url":"/notebooks/dask-xarray#how-to-access-underlying-data-in-an-xarray-object","position":30},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"How to access underlying data in an Xarray object?"},"content":"\n\nThere are two basic ways to extract values from an Xarray object:\n\nUsing .data will return a Dask array. For example:\n\ntref.data\n\nThis means that for Dask-backed Xarray object, we can access the values using .compute\n\n%%time\n\ntref.data.compute()\n\nWe can also use .values to see the “real” values of Xarray object. Another option is using .to_numpy. Both of these option return the values of underlying Dask object in a numpy array.\n\n%%time\ntref.to_numpy()\n\n","type":"content","url":"/notebooks/dask-xarray#how-to-access-underlying-data-in-an-xarray-object","position":31},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Computation"},"type":"lvl2","url":"/notebooks/dask-xarray#computation","position":32},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Computation"},"content":"\n\nAll built-in Xarray methods (.mean, .max, .rolling, .groupby etc.) support dask arrays.\n\nNow, let’s do some computations on this Xarray dataset.\n\n","type":"content","url":"/notebooks/dask-xarray#computation","position":33},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Single Point Calculations","lvl2":"Computation"},"type":"lvl3","url":"/notebooks/dask-xarray#single-point-calculations","position":34},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Single Point Calculations","lvl2":"Computation"},"content":"To start out, let’s do the calculations on a single point first. First, we extract the time series data at a grid point and save it to a variable. Here we select the closest point using .sel and load the data.\n\ntref_boulder = tref.sel(lat=40.0150, lon=-105.2705, method='nearest').load()\n\nWARNING: Remember as soon as you call .load() you are telling Dask to trigger computation.\n\n# -- take annual average\ntb = tref_boulder.resample(time='AS').mean()\ntb\n\nWe can either see the values of our DataArray in the text representation above or by plotting it:\n\ntb.plot()\n\n","type":"content","url":"/notebooks/dask-xarray#single-point-calculations","position":35},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Calculations over all grids","lvl2":"Computation"},"type":"lvl3","url":"/notebooks/dask-xarray#calculations-over-all-grids","position":36},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Calculations over all grids","lvl2":"Computation"},"content":"\n\n# change the unit from Kelvin to degree Celsius \ntref_c = tref - 273.15\ntref_c\n\n%%time \n\ntref_c = tref_c.load()\n\n%%time\n\n# Compute monthly anomaly\n\n# -- 1. calculate monthly average\ntref_grouped = tref.groupby('time.month')\ntmean = tref_grouped.mean(dim='time')\n\n#-- 2. calculate monthly anomaly\ntos_anom = tref_grouped - tmean\ntos_anom\n\n%%time\ncomputed_anom = tos_anom.load()\ntype(computed_anom)\n\ntos_anom.sel(lon=310, lat=50, method='nearest').plot( size=4)\n\ntos_anom.sel(time='2030-01-01').plot()\n\nTIP: Using Xarray plotting functionality automatically triggers computations on the Dask Array, similar to .compute().\n\nWe can do more complex calculations too:\n\nrolling_mean = tref.rolling(time=5).mean()\nrolling_mean  # contains dask array\n\ntimeseries = rolling_mean.isel(lon=1, lat=20)  # no activity on dashboard\ntimeseries  # contains dask array\n\ncomputed = rolling_mean.compute()  # activity on dashboard\ncomputed  # has real numpy values\n\n","type":"content","url":"/notebooks/dask-xarray#calculations-over-all-grids","position":37},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Supplementary Material: Advanced workflows and automatic parallelization using apply_ufunc","lvl2":"Computation"},"type":"lvl3","url":"/notebooks/dask-xarray#supplementary-material-advanced-workflows-and-automatic-parallelization-using-apply-ufunc","position":38},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Supplementary Material: Advanced workflows and automatic parallelization using apply_ufunc","lvl2":"Computation"},"content":"Most of xarray’s built-in operations work on Dask arrays. If you want to use a function that isn’t wrapped by Xarray to work with Dask, one option is to extract Dask arrays from xarray objects (.data) and use Dask directly.\n\nAnother option is to use xarray’s apply_ufunc() function. xr.apply_ufunc() can automate embarrassingly parallel “map” type operations where a function written for processing NumPy arrays, but we want to apply it on our Xarray DataArray.\n\nxr.apply_ufunc() give users capability to run custom-written functions such as parameter calculations in a parallel way. See the \n\nXarray tutorial material on apply_ufunc for more.\n\nIn the example below, we calculate the saturation vapor pressure by using apply_ufunc() to apply this function to our Dask Array chunk by chunk.\n\nimport numpy as np\n\ndef sat_p(t):\n    \"\"\"Calculate saturation vapor pressure using Clausius-Clapeyron equation\"\"\"\n    return 0.611 * np.exp(17.67 * (t-273.15)*((t-29.65)**(-1)))\n\nes = xr.apply_ufunc(sat_p, tref, dask=\"parallelized\", output_dtypes=[float])\nes\n\nes.compute()\n\nThe data used for this tutorial is from one ensemble member. What if we want to use multiple ensemble members? So far, we only run on one machine, what if we run an HPC cluster? We will go over this in the next tutorial.\n\n","type":"content","url":"/notebooks/dask-xarray#supplementary-material-advanced-workflows-and-automatic-parallelization-using-apply-ufunc","position":39},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Dask + Xarray Good Practices","lvl2":"Computation"},"type":"lvl3","url":"/notebooks/dask-xarray#dask-xarray-good-practices","position":40},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Dask + Xarray Good Practices","lvl2":"Computation"},"content":"Summary of Dask + Xarray Good Practices\n\nThe good practices regarding Dask + Xarray is the same as the good practices for Dask only.\n\nSimilar to Dask DataFrames, it is more efficient to first do spatial and temporal indexing (e.g. .sel() or .isel()) and filter the dataset early in the pipeline, especially before calling resample() or groupby().\n\nChunk sizes should be small enough to fit into the memory at once but large enough to avoid the additional communication overhead. Good chunk size ~100 MB.\n\nIt is always better to chunk along the time dimension.\n\nAvoid too many tasks since each task will introduce 1ms of overhead.\n\nWhen possible, use xr.apply_ufunc to apply an unvectorized function to the Xarray object.","type":"content","url":"/notebooks/dask-xarray#dask-xarray-good-practices","position":41},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Close you local Dask Cluster","lvl2":"Computation"},"type":"lvl3","url":"/notebooks/dask-xarray#close-you-local-dask-cluster","position":42},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Close you local Dask Cluster","lvl2":"Computation"},"content":"It is always a good practice to close the Dask cluster you created.\n\nclient.shutdown()\n\n","type":"content","url":"/notebooks/dask-xarray#close-you-local-dask-cluster","position":43},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/dask-xarray#summary","position":44},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Summary"},"content":"In this notebook, we have learned about:\n\nUsing Dask with Xarray\n\nRead/write netCDF files with Dask\n\nDask backed Xarray objects and operations\n\nExtract Dask arrays from Xarray objects and use Dask array directly..\n\nCustomized workflows using apply_ufunc","type":"content","url":"/notebooks/dask-xarray#summary","position":45},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/dask-xarray#resources-and-references","position":46},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl2":"Resources and references"},"content":"","type":"content","url":"/notebooks/dask-xarray#resources-and-references","position":47},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Reference","lvl2":"Resources and references"},"type":"lvl3","url":"/notebooks/dask-xarray#reference","position":48},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Reference","lvl2":"Resources and references"},"content":"Dask Array Docs\n\nDask Examples\n\nDask Code\n\nDask Blog\n\nXarray Docs\n\nXarray + Dask docs, particularly the \n\nbest practices\n\nXarray Tutorial material","type":"content","url":"/notebooks/dask-xarray#reference","position":49},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Ask for help","lvl2":"Resources and references"},"type":"lvl3","url":"/notebooks/dask-xarray#ask-for-help","position":50},{"hierarchy":{"lvl1":"Parallelizing Xarray with Dask","lvl3":"Ask for help","lvl2":"Resources and references"},"content":"dask tag on Stack Overflow, for usage questions\n\ngithub discussions: dask for general, non-bug, discussion, and usage questions\n\ngithub issues: dask for bug reports and feature requests\n\ngithub discussions: xarray for general, non-bug, discussion, and usage questions\n\ngithub issues: xarray for bug reports and feature requests","type":"content","url":"/notebooks/dask-xarray#ask-for-help","position":51},{"hierarchy":{"lvl1":"Dask Schedulers"},"type":"lvl1","url":"/notebooks/dask-cluster","position":0},{"hierarchy":{"lvl1":"Dask Schedulers"},"content":"","type":"content","url":"/notebooks/dask-cluster","position":1},{"hierarchy":{"lvl1":"Dask Schedulers"},"type":"lvl1","url":"/notebooks/dask-cluster#dask-schedulers","position":2},{"hierarchy":{"lvl1":"Dask Schedulers"},"content":"","type":"content","url":"/notebooks/dask-cluster#dask-schedulers","position":3},{"hierarchy":{"lvl1":"Dask Schedulers","lvl3":"In this tutorial, you learn:"},"type":"lvl3","url":"/notebooks/dask-cluster#in-this-tutorial-you-learn","position":4},{"hierarchy":{"lvl1":"Dask Schedulers","lvl3":"In this tutorial, you learn:"},"content":"Components of Dask Schedulers\n\nTypes of Dask Schedulers\n\nSingle Machine Schedulers\n\nRelated Documentation\n\nDask Scheduling\n\nDask Local Cluster\n\nDeploying Dask Cluster Manager\n\n","type":"content","url":"/notebooks/dask-cluster#in-this-tutorial-you-learn","position":5},{"hierarchy":{"lvl1":"Dask Schedulers","lvl2":"Introduction"},"type":"lvl2","url":"/notebooks/dask-cluster#introduction","position":6},{"hierarchy":{"lvl1":"Dask Schedulers","lvl2":"Introduction"},"content":"As we mentioned in our Dask overview, Dask is composed of two main parts:\n\nDask Collections (APIs)\n\nDynamic Task Scheduling\n\nSo far, we have talked about different Dask collections, but in this tutorial we are going to talk more about the second part.","type":"content","url":"/notebooks/dask-cluster#introduction","position":7},{"hierarchy":{"lvl1":"Dask Schedulers","lvl2":"The Dask scheduler - our task orchestrator"},"type":"lvl2","url":"/notebooks/dask-cluster#the-dask-scheduler-our-task-orchestrator","position":8},{"hierarchy":{"lvl1":"Dask Schedulers","lvl2":"The Dask scheduler - our task orchestrator"},"content":"The Dask.distributed task scheduler is a centralized, dynamic system that coordinates the efforts of various dask worker processes spread accross different machines.\n\nWhen a computational task is submitted, the Dask distributed scheduler sends it off to a Dask cluster - simply a collection of Dask workers. A worker is typically a separate Python process on either the local host or a remote machine.\n\nTo perform work, a scheduler must be assigned resources in the form of a Dask cluster. The cluster consists of the following components:\n\nscheduler : A scheduler creates and manages task graphs and distributes tasks to workers.\n\nworkers : A worker is typically a separate Python process on either the local host or a remote machine. A Dask cluster usually consists of many workers. Basically, a worker is a Python interpretor which will perform work on a subset of our dataset.\n\nclient - A high-level interface that points to the scheduler (often local but not always). A client serves as the entry point for interacting with a Dask scheduler.\n\n\n\nImage credit: Dask Contributors\n\n","type":"content","url":"/notebooks/dask-cluster#the-dask-scheduler-our-task-orchestrator","position":9},{"hierarchy":{"lvl1":"Dask Schedulers","lvl2":"Schedulers"},"type":"lvl2","url":"/notebooks/dask-cluster#schedulers","position":10},{"hierarchy":{"lvl1":"Dask Schedulers","lvl2":"Schedulers"},"content":"Dask essentially offers two types of schedulers:\n\nImage credit: Dask Contributors","type":"content","url":"/notebooks/dask-cluster#schedulers","position":11},{"hierarchy":{"lvl1":"Dask Schedulers","lvl3":"1. Single machine scheduler","lvl2":"Schedulers"},"type":"lvl3","url":"/notebooks/dask-cluster#id-1-single-machine-scheduler","position":12},{"hierarchy":{"lvl1":"Dask Schedulers","lvl3":"1. Single machine scheduler","lvl2":"Schedulers"},"content":"The Single-machine Scheduler schedules tasks and manages the execution of those tasks on the same machine where the scheduler is running.\n\nIt is designed to be used in situations where the amount of data or the computational requirements are too large for a single process to handle, but not large enough to warrant the use of a cluster of machines.\n\nIt is relatively simple and cheap to use but it does not scale as it only runs on a single machine.\n\nSingle machine scheduler is the default choice used by Dask.\n\nIn Dask, there are several types of single machine schedulers that can be used to schedule computations on a single machine:","type":"content","url":"/notebooks/dask-cluster#id-1-single-machine-scheduler","position":13},{"hierarchy":{"lvl1":"Dask Schedulers","lvl4":"1.1. Single-threaded scheduler","lvl3":"1. Single machine scheduler","lvl2":"Schedulers"},"type":"lvl4","url":"/notebooks/dask-cluster#id-1-1-single-threaded-scheduler","position":14},{"hierarchy":{"lvl1":"Dask Schedulers","lvl4":"1.1. Single-threaded scheduler","lvl3":"1. Single machine scheduler","lvl2":"Schedulers"},"content":"This scheduler runs all tasks serially on a single thread.This is only useful for debugging and profiling, but does not have any parallelization.","type":"content","url":"/notebooks/dask-cluster#id-1-1-single-threaded-scheduler","position":15},{"hierarchy":{"lvl1":"Dask Schedulers","lvl4":"1.2. Threaded scheduler","lvl3":"1. Single machine scheduler","lvl2":"Schedulers"},"type":"lvl4","url":"/notebooks/dask-cluster#id-1-2-threaded-scheduler","position":16},{"hierarchy":{"lvl1":"Dask Schedulers","lvl4":"1.2. Threaded scheduler","lvl3":"1. Single machine scheduler","lvl2":"Schedulers"},"content":"The threaded scheduler uses a pool of local threads to execute tasks concurrently.This is the default scheduler for Dask, and is suitable for most use cases on a single machine. Multithreading works well for Dask Array and Dask DataFrame.\n\nTo select one of the above scheduler for your computation, you can specify it when doing .compute():\n\nFor example:this.compute(scheduler=\"single-threaded\")  # for debugging and profiling only\n\nAs mentioned above the threaded scheduler is the default scheduler in Dask. But you can set the default scheduler to Single-threaded or multi-processing by:import dask\ndask.config.set(scheduler='synchronous')  # overwrite default with single-threaded scheduler\n\nMulti-processing works well for pure Python code - delayed functions and operations on Dask Bags.\n\nLet’s compare the performance of each of these single-machine schedulers:\n\nimport numpy as np\nimport dask.array as da\n\n%%time\n## - numpy performance\nxn = np.random.normal(10, 0.1, size=(20_000, 20_000))\nyn = xn.mean(axis=0)\nyn\n\n%%time\n# -- dask array using the default\nxd = da.random.normal(10, 0.1, size=(20_000, 20_000), chunks=(2000, 2000))\nyd = xd.mean(axis=0)\nyd.compute()\n\nimport time\n# -- dask testing different schedulers:\nfor sch in ['threading', 'processes', 'sync']:\n    t0 = time.time()\n    r = yd.compute(scheduler=sch)\n    t1 = time.time()\n    print(f\"{sch:>10} :  {t1 - t0:0.4f} s\")\n\nyd.dask\n\nyd\n\nNotice how sync scheduler takes almost the same time as pure NumPy code.\n\nWhy is the multiprocessing scheduler so much slower?\n\nIf you use the multiprocessing backend, all communication between processes still needs to pass through the main process because processes are isolated from other processes. This introduces a large overhead.\n\nThe Dask developers recommend using the Dask Distributed Scheduler which we will cover now.\n\n","type":"content","url":"/notebooks/dask-cluster#id-1-2-threaded-scheduler","position":17},{"hierarchy":{"lvl1":"Dask Schedulers","lvl3":"2. Distributed scheduler","lvl2":"Schedulers"},"type":"lvl3","url":"/notebooks/dask-cluster#id-2-distributed-scheduler","position":18},{"hierarchy":{"lvl1":"Dask Schedulers","lvl3":"2. Distributed scheduler","lvl2":"Schedulers"},"content":"The Distributed scheduler or dask.distributed schedules tasks and manages the execution of those tasks on workers from a single or multiple machines.\n\nThis scheduler is more sophisticated and offers more features including a live diagnostic dashboard which provides live insight on performance and progress of the calculations.\n\nIn most cases, dask.distributed is preferred since it is very scalable, and provides and informative interactive dashboard and access to more complex Dask collections such as futures.\n\n","type":"content","url":"/notebooks/dask-cluster#id-2-distributed-scheduler","position":19},{"hierarchy":{"lvl1":"Dask Schedulers","lvl4":"2.1. Local Cluster","lvl3":"2. Distributed scheduler","lvl2":"Schedulers"},"type":"lvl4","url":"/notebooks/dask-cluster#id-2-1-local-cluster","position":20},{"hierarchy":{"lvl1":"Dask Schedulers","lvl4":"2.1. Local Cluster","lvl3":"2. Distributed scheduler","lvl2":"Schedulers"},"content":"A Dask Local Cluster refers to a group of worker processes that run on a single machine and are managed by a single Dask scheduler.\n\nThis is useful for situations where the computational requirements are not large enough to warrant the use of a full cluster of separate machines. It provides an easy way to run parallel computations on a single machine, without the need for complex cluster management or other infrastructure.\n\n","type":"content","url":"/notebooks/dask-cluster#id-2-1-local-cluster","position":21},{"hierarchy":{"lvl1":"Dask Schedulers","lvl5":"Let’s start by creating a Local Cluster","lvl4":"2.1. Local Cluster","lvl3":"2. Distributed scheduler","lvl2":"Schedulers"},"type":"lvl5","url":"/notebooks/dask-cluster#lets-start-by-creating-a-local-cluster","position":22},{"hierarchy":{"lvl1":"Dask Schedulers","lvl5":"Let’s start by creating a Local Cluster","lvl4":"2.1. Local Cluster","lvl3":"2. Distributed scheduler","lvl2":"Schedulers"},"content":"For this we need to set up a LocalCluster using dask.distributed and connect a client to it.\n\nfrom dask.distributed import LocalCluster, Client\n\ncluster = LocalCluster()\nclient = Client(cluster)\nclient\n\n☝️ Click the Dashboard link above.\n\n👈 Or click the “Search” 🔍 button in the dask-labextension dashboard.\n\nIf no arguments are specified in LocalCluster it will automatically detect the number of CPU cores your system has and the amount of memory and create workers to appropriately fill that.\n\nA LocalCluster will use the full resources of the current JupyterLab session.  For example, if you used BinderHub, it will use the number of CPUs selected.\n\nNote that LocalCluster() takes a lot of optional arguments, allowing you to configure the number of processes/threads, memory limits and other settings.\n\nYou can also find your cluster dashboard link using :\n\ncluster.dashboard_link\n\n%%time\n# -- dask array using the default\nxd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\nyd = xd.mean(axis=0)\nyd.compute()\n\nAlways remember to close your local Dask cluster:\n\nclient.shutdown()\n\n","type":"content","url":"/notebooks/dask-cluster#lets-start-by-creating-a-local-cluster","position":23},{"hierarchy":{"lvl1":"Dask Schedulers","lvl3":"Dask Distributed (Cluster Managers)","lvl2":"Schedulers"},"type":"lvl3","url":"/notebooks/dask-cluster#dask-distributed-cluster-managers","position":24},{"hierarchy":{"lvl1":"Dask Schedulers","lvl3":"Dask Distributed (Cluster Managers)","lvl2":"Schedulers"},"content":"So far we have talked about running a job on a local machine.\n\nDask can be deployed on distributed infrastructure, such as a an HPC system or a cloud computing system.\n\nImage credit: Dask Contributors\n\nDask Cluster Managers have different names corresponding to different computing environments. Some examples are dask-jobqueue for your HPC systems (including PBSCluster) or Kubernetes Cluster for machines on the Cloud.\n\nThe NCAR tutorial series includes an in-depth exploration and practical use cases of Dask on HPC systems and best practices for Dask on HPC. For the complete set of NCAR tutorial materials, please refer to the main NCAR tutorial content available \n\nhere.\n\nFor more information visit the \n\nDask Docs.","type":"content","url":"/notebooks/dask-cluster#dask-distributed-cluster-managers","position":25},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in Project Pythia’s Dask Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1}]}