{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990cfa4c-2117-4435-9806-ff9048890398",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NCAR/dask-tutorial/main/images/NCAR-contemp-logo-blue.png\"\n",
    "     width=\"750px\"\n",
    "     alt=\"NCAR logo\"\n",
    "     style=\"vertical-align:middle;margin:30px 0px\"/>\n",
    "\n",
    "# Dask DataFrame\n",
    "\n",
    "\n",
    "**ESDS dask tutorial | 06 February, 2023**  \n",
    "\n",
    "Negin Sobhani, Brian Vanderwende, Deepak Cherian, Ben Kirk  \n",
    "Computational & Information Systems Lab (CISL)  \n",
    "[negins@ucar.edu](mailto:negins@ucar.edu), [vanderwb@ucar.edu](mailto:vanderwb@ucar.edu)\n",
    "\n",
    "---------\n",
    "### In this tutorial, you learn:\n",
    "\n",
    "* Basic concepts and features of Dask DataFrames\n",
    "* Applications of Dask DataFrames\n",
    "* Interacting with Dask DataFrames\n",
    "* Built-in operations with Dask DataFrames\n",
    "* Dask DataFrames Best Practices\n",
    "\n",
    "### Related Documentation\n",
    "\n",
    "* [Dask DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [Dask DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [Dask DataFrame examples](https://examples.dask.org/dataframe.html)\n",
    "* [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NCAR/dask-tutorial/main/images/dask_dataframe.png\"\n",
    "     align=\"right\"\n",
    "     width=\"530px\"\n",
    "     alt=\"Dask DataFrame is composed of pandas DataFrames\"/>\n",
    "     \n",
    "     \n",
    "\n",
    "pandas is a very popular tool for working with **tabular datasets**, but the dataset needs to **fit into the memory**. \n",
    "\n",
    "pandas operates best with smaller datasets, and if you have a large dataset, you’ll receive an out of memory error using pandas. A general rule of thumb for pandas is:\n",
    "\n",
    "> \"Have 5 to 10 times as much RAM as the size of your dataset\"\n",
    ">\n",
    "> ~ Wes McKinney (2017) in [10 things I hate about pandas](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)\n",
    "\n",
    "But Dask DataFrame can be used to solve pandas performance issues with larger-than-memory datasets.\n",
    "\n",
    "\n",
    "### What is Dask DataFrame?\n",
    "\n",
    "\n",
    "* **A Dask DataFrame is a parallel DataFrame composed of smaller pandas DataFrames (also known as *partitions*).**\n",
    "\n",
    "* Dask Dataframes look and feel like the pandas DataFrames on the surface. \n",
    "\n",
    "* Dask DataFrames partition the data into manageable **partitions** that can be processed in parallel and across multiple cores or computers. \n",
    "\n",
    "* Similar to Dask Arrays, Dask DataFrames are lazy!\n",
    "\n",
    "    Unlike pandas, operations on Dask DataFrames are not computed until you explicitly request them (e.g. by calling `.compute`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae50738-0e67-4a64-8190-12ab8c06e06c",
   "metadata": {},
   "source": [
    "## When to use Dask DataFrame and when to avoid it?\n",
    "\n",
    "Dask DataFrames are used in situations where pandas *fails* or has *poor performance due to data size*.  \n",
    "\n",
    "Dask DataFrame is a good choice when doing **parallalizeable computations**.  \n",
    "Some examples are:\n",
    "* Element-wise operations such as `df.x + df.y`\n",
    "* Row-wise filtering such as `df[df.x>0]`\n",
    "* Common aggregations such as `df.x.max()`\n",
    "* Dropping duplicates such as `df.x.drop_duplicate()`\n",
    "\n",
    "However, Dask is not great for operations that requires shuffling or re-indexing.  \n",
    "Some examples are:\n",
    "* Set index: `df.set_index(df.x)`\n",
    "\n",
    "\n",
    " \n",
    "<div class=\"alert alert-block alert-warning\" markdown=\"1\">\n",
    "<b>WARNING:</b> Although, Dask DataFrame has a very similar interface to the pandas DataFrame (as we will see in this tutorial), it does NOT include some of the pandas interface yet.\n",
    "\n",
    "See the [Dask DataFrame API documentation](https://docs.dask.org/en/stable/dataframe-api.html) for a compehnsive list of available functions. \n",
    "</div>\n",
    "\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17323017-5697-458a-a2e0-f13af6262872",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tutorial Dataset\n",
    "In this tutorial, we are going to use the NOAA Global Historical Climatology Network Daily (GHCN-D) dataset.  \n",
    "GHCN-D is a public available dataset that includes daily climate records from +100,000 surface observations around the world.  \n",
    "This is an example of a real dataset that is used by NCAR scientists for their research. GHCN-D raw dataset for all stations is available through [NOAA Climate Data Online](https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND). \n",
    "\n",
    "**To learn more about GHCNd dataset, please visit:**\n",
    "* [GHCNd Journal Paper](https://journals.ametsoc.org/view/journals/atot/29/7/jtech-d-11-00103_1.xml)\n",
    "* [GHCNd Official Website](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily)\n",
    "\n",
    "### Download the data\n",
    "For this example, we are going to look through a subset of data from the GHCN-D dataset.\n",
    "\n",
    "First, we look at the daily observations from Denver International Airport, next we are going to look through selected stations in the US. \n",
    "\n",
    "The access the preprocessed dataset for this tutorial, please run the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36376778-77b8-4fe9-b370-6a76a335fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./get_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfddcf-0f62-4f8b-bbfa-50250ac5a6c4",
   "metadata": {},
   "source": [
    "This script should save the preprocessed GHCN-D data in `../data` path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cc4275-c0ce-42fa-a6b4-033c597b5c9c",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Pandas DataFrame Basics\n",
    "Let's start with an example using pandas DataFrame.\n",
    "\n",
    "First, let's read in the comma-seperated GHCN-D dataset for one station at **Denver International Airport (DIA), CO** (site ID : `USW00003017`).\n",
    "\n",
    "To see the list of all available GHCN-D sites and their coordinates and IDs, please see [this link](https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5337d-28c3-4f88-b9c3-ce3d92445c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# DIA ghcnd id\n",
    "site = 'USW00003017'\n",
    "data_dir = '../data/'\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, site+'.csv'), parse_dates=['DATE'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fa1c8-a550-4fd1-8418-cae2844fe657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top five rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc065d-06e1-4cd5-85b4-9d30eb742f6d",
   "metadata": {},
   "source": [
    "**Question:** What variables are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b80f27-b70f-43bd-af0d-85def9d2cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b5e52-31f8-422b-8f02-05535c064f8f",
   "metadata": {},
   "source": [
    "The description and units of the dataset is available [here](https://www.ncei.noaa.gov/pub/data/ghcn/daily/readme.txt).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c6ad4-51fe-4e65-8b07-542eccf0c6b8",
   "metadata": {},
   "source": [
    "### Operations on pandas DataFrame\n",
    "\n",
    "pandas DataFrames has several features that give us flexibility to do different calculations and analysis on our dataset. Let's check some out:\n",
    "#### Simple Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27a505-3ceb-469a-b6aa-9ded4f74e16e",
   "metadata": {},
   "source": [
    "For example: \n",
    "* When was the coldest day at this station during December of last year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd3450-904c-44a0-820a-266cc1327871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use python slicing notation inside .loc \n",
    "# use idxmin() to find the index of minimum valus\n",
    "df.loc['2022-12-01':'2022-12-31'].TMIN.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c1ae0-49bf-4d47-b797-a65c31f9c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we easily plot the prior data using matplotlib from pandas\n",
    "# -- .loc for value based indexing\n",
    "df.loc['2022-12-01':'2022-12-31'].SNWD.plot(ylabel= 'Daily Average Snow Depth [mm]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc3daf-896d-482e-bbfe-1d5dfe45c049",
   "metadata": {},
   "source": [
    "* How many snow days do we have each year at this station?\n",
    "\n",
    "Pandas groupby is used for grouping the data according to the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10c3ff-4dff-4e5a-aa1e-059583272506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- First select days with snow > 0\n",
    "# 2- Create a \"groupby object\" based on the selected columns\n",
    "# 3- use .size() to compute the size of each group\n",
    "# 4- sort the values descending \n",
    "\n",
    "# we count days where SNOW>0, and sort them and show top 5 years:\n",
    "df[df['SNOW']>0].groupby('YEAR').size().sort_values(ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539216a-6cf8-4b6f-bc48-d9f670ef6176",
   "metadata": {},
   "source": [
    "Or for a more complex analysis:\n",
    "\n",
    "For example, we have heard that this could be Denver's first January in 13 years with no 60-degree days.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NCAR/dask-tutorial/main/images/denver2.png\"\n",
    "     alt=\"Dask DataFrame is composed of pandas DataFrames\"/>\n",
    "\n",
    "Below, we show all days with high temperature above 60°F (155.5°C/10) since 2010:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c25614-b47c-4baa-b429-8e3f17e3a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['MONTH']==1) & (df['YEAR']>=2010) & (df['TMAX']>155.5)].groupby(['YEAR']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb54cb-9358-4ef9-b913-a8f653993ae5",
   "metadata": {},
   "source": [
    "This is great! But how big is this dataset for one station?\n",
    "\n",
    "First, let's check the file size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb041314-2253-4933-ab2c-c81f742f44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh ../data/USW00003017.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84f8f2-94f1-45d0-b789-82c779aa3ce0",
   "metadata": {},
   "source": [
    "Similar to the previous tutorial, we can use the following function to find the size of a variable on memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d54fc-ec05-41c1-a624-274e40772d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to display variable size in MB\n",
    "import sys\n",
    "def var_size(in_var):\n",
    "    result = sys.getsizeof(in_var) / 1e6\n",
    "    print(f\"Size of variable: {result:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c0b151-1454-4e38-96c6-41c38434b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_size(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae6ce6-0dca-47f1-bf84-ca3b46bfbb91",
   "metadata": {},
   "source": [
    "Remember, the above rule?\n",
    "\n",
    "> \"Have 5 to 10 times as much RAM as the size of your dataset\"\n",
    ">\n",
    "> ~ Wes McKinney (2017) in [10 things I hate about pandas](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0ba8b-8c65-40ce-81cb-0c313177fc8b",
   "metadata": {},
   "source": [
    "**So far, we read in and analyzed data for one station. We have a total of +118,000 stations over the world and +4500 stations in Colorado alone!**\n",
    "\n",
    "**What if we want to look at the larger dataset?**\n",
    "\n",
    "## Scaling up to a larger dataset\n",
    "\n",
    "Let's start by reading data from selected stations. The downloaded data for this example includes the climatology observations from 66 selected sites in Colorado.\n",
    "\n",
    "Pandas can concatenate data to load data spread across multiple files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0edd0-a58a-4ad4-8ab7-551122c92d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -csh ../data/*.csv |tail -n1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1258cc3-d24a-4268-ad19-62e02905b635",
   "metadata": {},
   "source": [
    "Using a for loop with `pandas.concat`, we can read multiple files at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc9c27-00da-426d-bb45-83f959f411ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import glob\n",
    "co_sites = glob.glob(os.path.join(data_dir, '*.csv'))\n",
    "df = pd.concat(pd.read_csv(f, index_col=0, parse_dates=['DATE']) for f in co_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16de06-b7f5-4d4a-ac71-011d7ce5e1b1",
   "metadata": {},
   "source": [
    "* How many stations have we read in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4441c14e-a51b-4f59-b7e9-2fa0bdb1f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Concatenated data for\", len(df.ID.unique()), \"unique sites.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab38e3b-1c37-48f8-86ca-903192cfcc1f",
   "metadata": {},
   "source": [
    "Now that we concatenated the data for all sites in one DataFrame, we can do similar analysis on it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fee981-ef7c-4bbb-b7a2-db4705b3902e",
   "metadata": {},
   "source": [
    "* Which site has recorded the most snow days in a year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509a4e3-ab07-4bd5-bbb8-fe121e481891",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ~90s on 4GB RAM\n",
    "snowy_days = df[df['SNOW']>0].groupby(['ID','YEAR']).size()\n",
    "\n",
    "print ('This site has the highest number of snow days in a year : ')\n",
    "snowy_days.agg(['idxmax','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a297c-96cb-4014-a196-72e1482901a9",
   "metadata": {},
   "source": [
    "**Excersise:** Which Colorado site has recorded the most snow days in 2023?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd5656-49a3-448c-b339-c6844304f61f",
   "metadata": {},
   "source": [
    "**Dask allows us to conceptualize all of these files as a single dataframe!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aff343-4431-4224-9c82-9ed725c9ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a little cleanup\n",
    "del df, snowy_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3746273b-4057-466d-b7c9-a78932cd5808",
   "metadata": {},
   "source": [
    "## Computations on Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1339a-7ee4-4125-b281-c467358d79a5",
   "metadata": {},
   "source": [
    "### Create a \"LocalCluster\" Client with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb859ca6-a98d-40cd-a389-289c5246f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ae984-5763-4360-ab1b-2ecf92500952",
   "metadata": {},
   "source": [
    "☝️ Click the Dashboard link above.\n",
    "\n",
    "👈 Or click the “Search” 🔍 button in the dask-labextension dashboard.\n",
    "\n",
    "### Dask DataFrame `read_csv` to read multiple files\n",
    "\n",
    "`dask.dataframe.read_csv` function can be used in conjunction with `glob` to read multiple csv files at the same time. \n",
    "\n",
    "Remember we can read one file with `pandas.read_csv`. For reading multiple files with pandas, we have to concatenate them with `pd.concatenate`. However, we can read many files at once just using `dask.dataframe.read_csv`.\n",
    "\n",
    "Overall, Dask is designed to perform I/O in parallel and is more performant than pandas for operations with multiple files or large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63add1-7e7f-4631-ae82-19800f4122a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dd.read_csv(co_sites, parse_dates=['DATE'])\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8147b02-8f8d-4591-92c7-be3441bea8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.TMAX.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a8028-c8f0-4bcf-8841-d4506fafb9d7",
   "metadata": {},
   "source": [
    "**Notice that the representation of the DataFrame object contains no data just headers and datatypes.  Why?**\n",
    "\n",
    "#### Lazy Evaluation\n",
    "\n",
    "Similar to Dask Arrays, Dask DataFrames are lazy. Here the data has not yet been read into the dataframe yet (a.k.a. lazy evaluation).  \n",
    "Dask just construct the task graph of the computation but it will \"evaluate\" them only when necessary.\n",
    "\n",
    "**So how does Dask know the name and dtype of each column?**\n",
    "\n",
    "Dask has just read the start of the first file and infers the column names and dtypes. \n",
    "\n",
    "Unlike `pandas.read_csv` that reads in all files before inferring data types, `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file if using a glob). The column names and dtypes are then enforced when reading the specific partitions (*Dask can make mistakes on these inferences if there is missing or misleading data in the early rows*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f062ee-ca69-4262-8316-807705a8fa07",
   "metadata": {},
   "source": [
    "Let's take a look at the start of our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4823d-a785-495d-b2f5-e09a539f48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb8ab3-444e-41e3-94a0-8ec1b6354751",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" markdown=\"1\">\n",
    "\n",
    "<b>NOTE:</b>  Whenever we operate on our dataframe we read through all of our CSV data so that we don’t fill up RAM. Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible. This enables you to handle larger than memory datasets but, repeated computations will have to load all of the data in each time.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c0a3a-f70a-44ee-912f-da92344c0fb2",
   "metadata": {},
   "source": [
    "Similar data manipulations as `pandas.dataframe` can be done for `dask.dataframes`.  \n",
    "For example, let's find the highest number of snow days in Colorado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c85cb-cb7b-41ad-b3aa-c5134c46e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print ('This site has the highest number of snow days in a year : ')\n",
    "snowy_days = ddf[ddf['SNOW']>0].groupby(['ID','YEAR']).size()\n",
    "snowy_days.compute().agg(['idxmax','max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6f60b-14a5-4816-8f98-c0f0619e3a32",
   "metadata": {},
   "source": [
    "#### Nice, but what did Dask do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c119c3-68df-457a-af03-fcee413e67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires ipywidgets\n",
    "\n",
    "snowy_days.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b624a-3418-44bb-95af-766a649ca328",
   "metadata": {},
   "source": [
    "You can also view the underlying task graph using `.visualize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f061feb-4e52-4b3e-a62f-cbadca2c06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph is too large\n",
    "snowy_days.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29864d-693b-4afe-abbf-2c6d350fc777",
   "metadata": {},
   "source": [
    "### Use `.compute` wisely!\n",
    "#### Share intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9e11d-2053-4758-a16f-901a540e8659",
   "metadata": {},
   "source": [
    "For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once.\n",
    "\n",
    "For example, let’s compute the mean and standard deviation for Maximum daily temperature of all snow days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d16724-c92c-459f-811a-7194dfb9bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowy_days = ddf[ddf['SNOW']>0]\n",
    "mean_tmax = snowy_days.TMAX.mean()\n",
    "std_tmax = snowy_days.TMAX.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1b711-c71c-4bec-a61e-3db9745db201",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_tmax_result = mean_tmax.compute()\n",
    "std_tmax_result = std_tmax.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b2065-f354-448f-a347-e52892c77dc6",
   "metadata": {},
   "source": [
    "But if we pass both arguments in a single `.compute`, we can share the intermediate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a657e65-e968-4c2f-b532-437798a10202",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_tmax_result, std_tmax_result = dask.compute(mean_tmax, std_tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c6c726-ab95-42c4-94a7-63ccaecdc34b",
   "metadata": {},
   "source": [
    "Here using `dask.compute` only one allowed sharing intermediate results between TMAX mean and median calculations and improved total performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a1fc0-7ef5-462d-befd-811291d60441",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tmax.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59987c79-7adb-4903-9d5d-5517abbab38c",
   "metadata": {},
   "source": [
    "Here some operations such as the calls to read the csv files, the filtering, and the grouping is exactly similar between both operations, so they can share intermediate results. Remember, Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f87712-b502-48bc-be7f-8c5623003c1f",
   "metadata": {},
   "source": [
    "### `.persist` or caching\n",
    "Sometimes you might want your computers to keep intermediate results in memory, if it fits in the memory. \n",
    "\n",
    "The `.persist()` method can be used to  “cache” data and tell Dask what results to keep around. You should only use `.persist()` with any data or computation that fits in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df12cb-bca1-4729-a2b4-3b3f51d4fcbb",
   "metadata": {},
   "source": [
    "For example, if we want to only do analysis on a subset of data (for example snow days at Boulder site):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf95ec2-6c65-4528-8f40-2ffac9f964f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boulder_snow = ddf[(ddf['SNOW']>0)&(ddf['ID']=='USC00050848')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf6554-c94a-436a-b603-3701bc30b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tmax = boulder_snow.TMAX.mean().compute()\n",
    "tmin = boulder_snow.TMIN.mean().compute()\n",
    "\n",
    "print (tmin, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c09219c-44b7-40fe-bfa2-0e5ab5c963cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "boulder_snow = ddf[(ddf['SNOW']>0)&(ddf['ID']=='USC00050848')].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d9a9d-2096-4a33-8c95-b2e830ac7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tmax = boulder_snow.TMAX.mean().compute()\n",
    "tmin = boulder_snow.TMIN.mean().compute()\n",
    "print (tmin, tmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02a943-72ec-4192-8ac5-1946b558fcc9",
   "metadata": {},
   "source": [
    "As you can see the analysis on this persisted data is much faster because we are not repeating the loading and selecting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453ad26-ce5c-42cc-8c08-2b40e844a22d",
   "metadata": {},
   "source": [
    "## Dask DataFrames Best Practices\n",
    "\n",
    "### Use pandas (when you can)\n",
    "For data that fits into RAM, pandas can often be easier and more efficient to use than Dask DataFrame. However, Dask DataFrame is a powerful tool for larger-than-memory datasets. \n",
    "\n",
    "When the data is still larger than memory, Dask DataFrame can be used to **reduce** the larger datasets to a manageable level that pandas can handle. Next, use pandas at that point.\n",
    "\n",
    "### Avoid Full-Data Shuffling\n",
    "Some operations are more expensive to compute in a parallel setting than if they are in-memory on a single machine (for example, `set_index` or `merge`). In particular, **shuffling** operations that rearrange data can become very communication intensive. \n",
    "\n",
    "\n",
    "### pandas performance tips\n",
    "\n",
    "pandas performance tips such as using vectorized operations also apply to Dask DataFrames. See [Modern Pandas notebook](https://github.com/TomAugspurger/effective-pandas/blob/master/modern_1_intro.ipynb) for more tips on better performance with pandas. \n",
    "\n",
    "\n",
    "### Check Partition Size\n",
    "\n",
    "Similar to chunks, partitions should be small enough that they fit in the memory, but large enough to avoid that the communication overhead. \n",
    "\n",
    "\n",
    "#### `blocksize`\n",
    "* The number of partitions can be set using the `blocksize` argument. \n",
    "If none is given, the number of partitions/blocksize is calculated depending on the available memory and the number of cores on a machine up to a max of 64 MB. As we increase the blocksize, the number of partitions (calculated by Dask) will decrease. This is especially important when reading one large csv file. \n",
    "\n",
    "\n",
    "**As a good rule of thumb, you should aim for partitions that have around 100MB of data each.**\n",
    "\n",
    "\n",
    "### Smart use of `.compute()`\n",
    "Try avoiding running `.compute()` operation as long as possible. Dask works best when users avoid computation until results are needed. The `.compute()` command informs Dask to trigger computations on the Dask DataFrame.  \n",
    "As shown in the above example, the intermediate results can also be shared by calling `.compute()` only once. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a91d09-116f-4ea3-b1e4-e7755d406b4a",
   "metadata": {},
   "source": [
    "### Close your local Dask Cluster\n",
    "It is always a good practice to close the Dask cluster you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd19bbe-d887-4fbb-b4eb-5dc0881e9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d12769-eaba-47fc-97e0-6d5a4298fbc4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have learned about:\n",
    "\n",
    "* Dask DataFrame concept and component.\n",
    "* When to use and when to avoid Dask DataFrames?\n",
    "* How to use Dask DataFrame?\n",
    "* Some best practices around Dask DataFrames.\n",
    "\n",
    "\n",
    "## Resources and references\n",
    "\n",
    "* Reference\n",
    "    *  [Dask Docs](https://dask.org/)\n",
    "    *  [Dask Examples](https://examples.dask.org/)\n",
    "    *  [Dask Code](https://github.com/dask/dask/)\n",
    "    *  [Dask Blog](https://blog.dask.org/)\n",
    "    *  [Pandas Docs](https://pandas.pydata.org/docs/)\n",
    "  \n",
    "*  Ask for help\n",
    "    *   [`dask`](http://stackoverflow.com/questions/tagged/dask) tag on Stack Overflow, for usage questions\n",
    "    *   [github discussions: dask](https://github.com/dask/dask/discussions) for general, non-bug, discussion, and usage questions\n",
    "    *   [github issues: dask](https://github.com/dask/dask/issues/new) for bug reports and feature requests\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
