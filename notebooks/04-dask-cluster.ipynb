{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990cfa4c-2117-4435-9806-ff9048890398",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/NCAR/dask-tutorial/main/images/NCAR-contemp-logo-blue.png\"\n",
    "     width=\"750px\"\n",
    "     alt=\"NCAR logo\"\n",
    "     style=\"vertical-align:middle;margin:30px 0px\"/>\n",
    "\n",
    "# Dask Schedulers\n",
    "\n",
    "**ESDS dask tutorial | 06 February, 2023**  \n",
    "\n",
    "Negin Sobhani, Brian Vanderwende, Deepak Cherian, Ben Kirk  \n",
    "Computational & Information Systems Lab (CISL)  \n",
    "[negins@ucar.edu](mailto:negins@ucar.edu), [vanderwb@ucar.edu](mailto:vanderwb@ucar.edu)\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1fc96-8189-48b4-a61c-c8f2d2fc1a58",
   "metadata": {},
   "source": [
    "### In this tutorial, you learn:\n",
    "\n",
    "* Components of Dask Schedulers\n",
    "* Types of Dask Schedulers\n",
    "* Single Machine Schedulers\n",
    "\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Dask Scheduling](https://docs.dask.org/en/latest/scheduling.html)  \n",
    "* [Dask Local Cluster](https://docs.dask.org/en/stable/deploying-python.html)  \n",
    "* [Dask Cluster manager](https://docs.dask.org/en/latest/deploying-python.html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214569a5-8969-4e1f-8b24-d8d3183219a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "As we mentioned in our Dask overview, Dask is composed of two main parts:\n",
    "\n",
    "1. Dask Collections (APIs)\n",
    "2. Dynamic Task Scheduling\n",
    "\n",
    "So far, we have talked about different Dask collections, but in this tutorial we are going to talk more about the second part. \n",
    "\n",
    "\n",
    "## The Dask scheduler - our task orchestrator\n",
    "\n",
    "The `Dask.distributed` task *scheduler* is a centralized, dynamic system that coordinates the efforts of various dask *worker* processes spread accross different machines.\n",
    "\n",
    "When a computational task is submitted, the Dask distributed *scheduler* sends it off to a Dask *cluster* - simply a collection of Dask *workers*. A worker is typically a separate Python process on either the local host or a remote machine.  \n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\">\n",
    "\n",
    "* **worker** - a Python interpretor which will perform work on a subset of our dataset.\n",
    "* **cluster** - an object containing instructions for starting and talking to workers.\n",
    "* **scheduler** - sends tasks from our task graph to workers.\n",
    "* **client** - a local object that points to the scheduler (*often local but not always*). </div>\n",
    "\n",
    "<img src=\"https://docs.mlrun.org/en/stable/_images/dask_dist.png\" width=\"700px\" style=\"vertical-align:middle;margin:30px 0px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263fd4a3-0475-41a4-abae-417e8731100a",
   "metadata": {},
   "source": [
    "## Schedulers\n",
    "Dask essentially offers two types of schedulers:\n",
    "\n",
    "\n",
    "<img src=\"https://docs.mlrun.org/en/stable/_images/dask-overview.svg\" width=\"700px\" style=\"vertical-align:middle;margin:30px 0px\"/>\n",
    "\n",
    "### 1. Single machine scheduler \n",
    "* The Single-machine Scheduler schedules tasks and manages the execution of those tasks on the same machine where the scheduler is running.  \n",
    "* It is designed to be used in situations where the amount of data or the computational requirements are too large for a single process to handle, but not large enough to warrant the use of a cluster of machines.\n",
    "* It is relatively simple and cheap to use but it does not scale as it only runs on a single machine. \n",
    "\n",
    "**Single machine scheduler is the default choice used by Dask.**\n",
    "\n",
    "In Dask, there are several types of single machine schedulers that can be used to schedule computations on a single machine:\n",
    "#### 1.1. Single-threaded scheduler\n",
    "This scheduler runs all tasks **serially** on a single thread.  \n",
    "This is only useful for debugging and profiling, but does not have any parallelization. \n",
    "\n",
    "#### 1.2. Threaded scheduler\n",
    "The threaded scheduler uses a pool of **local** threads to execute tasks concurrently.  \n",
    "This is the default scheduler for Dask, and is suitable for most use cases on a single machine. Multithreading works well for Dask Array and Dask DataFrame. \n",
    "\n",
    "To select one of the above scheduler for your computation, you can specify it when doing `.compute()`:\n",
    "\n",
    "For example: \n",
    "```python\n",
    "this.compute(scheduler=\"single-threaded\")  # for debugging and profiling only\n",
    "```\n",
    "\n",
    "\n",
    "As mentioned above the *threaded scheduler* is the default scheduler in Dask. But you can set the default scheduler to Single-threaded or multi-processing by: \n",
    "\n",
    "```python\n",
    "import dask\n",
    "dask.config.set(scheduler='synchronous')  # overwrite default with single-threaded scheduler\n",
    "```\n",
    "\n",
    "Multi-processing works well for pure Python code - *delayed* functions and operations on Dask Bags.\n",
    "\n",
    "Let's compare the performance of each of these single-machine schedulers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a8b81-6474-47b0-8892-d0f5d65f0ffa",
   "metadata": {},
   "source": [
    "### Distributed Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed871a9-45d4-4b0e-9f39-4dbb0b871c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fb1fe-bbd7-4207-8ac6-8eb784d6710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## - numpy performance\n",
    "xn = np.random.normal(10, 0.1, size=(20_000, 20_000))\n",
    "yn = xn.mean(axis=0)\n",
    "yn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af609dc9-9482-41e0-8a93-5cbcf096e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# -- dask array using the default\n",
    "xd = da.random.normal(10, 0.1, size=(20_000, 20_000), chunks=(2000, 2000))\n",
    "yd = xd.mean(axis=0)\n",
    "yd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141053bf-942c-46be-b169-2709ca569451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# -- dask testing different schedulers:\n",
    "for sch in ['threading', 'processes', 'sync']:\n",
    "    t0 = time.time()\n",
    "    r = yd.compute(scheduler=sch)\n",
    "    t1 = time.time()\n",
    "    print(f\"{sch:>10} :  {t1 - t0:0.4f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33d184-7c25-4052-aece-50dd9382e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yd.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641646c-3175-422e-95aa-f0d638b0687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2398e2-9ac6-4e85-89fe-8c794571dfeb",
   "metadata": {},
   "source": [
    "* Notice how `sync` scheduler takes almost the same time as pure NumPy code. \n",
    "* Why is the multiprocessing scheduler so much slower?\n",
    "\n",
    "If you use the multiprocessing backend, all communication between processes still needs to pass through the main process because processes are isolated from other processes. This introduces a large overhead. \n",
    "\n",
    "**The Dask developers recommend using the Dask Distributed Scheduler which we will cover now.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292ca19-7d31-4702-ae86-ecb75c16a47d",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Distributed scheduler\n",
    "* The Distributed scheduler or `dask.distributed` schedules tasks and manages the execution of those tasks on workers from a single or multiple machines. \n",
    "* This scheduler is more sophisticated and offers more features including a live diagnostic dashboard which provides live insight on performance and progress of the calculations.\n",
    "\n",
    "\n",
    "In most cases, `dask.distributed` is preferred since it is very scalable, and provides and informative interactive dashboard and access to more complex Dask collections such as `futures`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d506bb-1c20-46f7-a0ea-2d9b10ffce6f",
   "metadata": {},
   "source": [
    "#### 2.1. Local Cluster\n",
    "\n",
    "A Dask Local Cluster refers to a group of worker processes that run on a single machine and are managed by a single Dask scheduler. \n",
    "\n",
    "This is useful for situations where the computational requirements are not large enough to warrant the use of a full cluster of separate machines. It provides an easy way to run parallel computations on a single machine, without the need for complex cluster management or other infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a0d22-e14f-46cc-9e54-157d098f5380",
   "metadata": {},
   "source": [
    "##### Let's start by creating a Local Cluster\n",
    "\n",
    "For this we need to set up a `LocalCluster` using `dask.distributed` and connect a `client` to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3e27b-7ac6-4250-8e01-3836fd7e0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3384ef-9a9a-4425-bd77-6ca811ff0577",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Click the Dashboard link above.\n",
    "\n",
    "üëà Or click the ‚ÄúSearch‚Äù üîç button in the dask-labextension dashboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556c2ff-333f-48bf-9183-689ff589f011",
   "metadata": {},
   "source": [
    "If no arguments are specified in `LocalCluster` it will automatically detect the number of CPU cores your system has and the amount of memory and create workers to appropriately fill that.\n",
    "\n",
    "A `LocalCluster` will use the full resources of the current JupyterLab session.  For example, if you used NCAR JupyterHub, it will use the number of CPUs selected. \n",
    "\n",
    "\n",
    "Note that `LocalCluster()` takes a lot of optional arguments, allowing you to configure the number of processes/threads, memory limits and other settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f0121-2307-4ea6-b3f7-9e7925c53ca8",
   "metadata": {},
   "source": [
    "You can also find your cluster dashboard link using : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9a540-2592-44fb-b77e-4474d8521125",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f41158-4165-4da8-a2d1-5afb68edf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# -- dask array using the default\n",
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
    "yd = xd.mean(axis=0)\n",
    "yd.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2d2b0-7cd9-4e25-98f4-46a801d29964",
   "metadata": {},
   "source": [
    "Always remember to close your local Dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a292434-3f33-480e-b332-be809f8c8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c031818-16fd-44ef-b94c-f0f1843567f7",
   "metadata": {},
   "source": [
    "### Dask Distributed (Cluster)\n",
    "\n",
    "So far we have talked about running a job on a local machine.\n",
    "\n",
    "Dask can be deployed on distributed infrastructure, such as a an HPC system or a cloud computing system.\n",
    "\n",
    "\n",
    "    \n",
    "Dask Clusters have different names corresponding to different computing environments. Some examples are `dask-jobqueue` for your HPC systems (including `PBSCluster`) or Kubernetes Cluster for machines on the Cloud. \n",
    "\n",
    "In section 5, we will talk more about Dask on HPC Systems. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
